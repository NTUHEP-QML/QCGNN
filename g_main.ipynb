{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main script\n",
    "\n",
    "This notebook script is about the main training procedure of MPGNN and QCGNN, with some functions and classes are written in other python scripts `module_*.py`. The following codes contain:\n",
    "- Setup for the classical MPGNN.\n",
    "- Data encoding ansatz of QCGNN (on simulator or IBMQ).\n",
    "- Training and prediction workflow.\n",
    "\n",
    "About the modules `module_*.py`:\n",
    "- `module_data.py`: Reading data and constructing data modules.\n",
    "- `module_model.py`: Some detail construction of classical and quantum models.\n",
    "- `module_training.py`: Related to routine training procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages\n",
    "\n",
    "- `lightning`: tools for simplified training procedure.\n",
    "- `pennylane`: used for quantum machine learning.\n",
    "- `torch`: machine learning backend (for both classical and quantum).\n",
    "- `torch_geometric`: used for classical graph neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from itertools import product\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import lightning as L\n",
    "import pandas as pd\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "import module_data\n",
    "import module_model\n",
    "import module_training\n",
    "\n",
    "# Faster calculation on GPU but less precision.\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "# See https://discuss.pennylane.ai/t/qml-prod-vs-direct-operators-product/3873\n",
    "qml.operation.enable_new_opmath()\n",
    "\n",
    "# QCGNN template to use\n",
    "QCGNN = module_model.QCGNN_IX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Settings\n",
    "\n",
    "Most of the configuration setup can be done in `./config.json`, some explanation of the arguments in `./config.json`.\n",
    "- Set `quick_test` to true to test whether the code can run in your environment.\n",
    "- Set `computing_platform` to \"slurm\" if running in *slurm* (optional).\n",
    "- Set `use_wandb` to true if you want to monitor the trainning procedure through [Wandb](https://wandb.ai/site), otherwise the training result will be saved as a `csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The logging function.\n",
    "def _log(message: str) -> None:\n",
    "    \"\"\"Printing function for log.\"\"\"\n",
    "    print(f\"# MainLog: {message}\")\n",
    "\n",
    "# Read json configuration file.\n",
    "with open(\"config.json\", \"r\") as json_file:\n",
    "    # This json config might be loaded for other use in other python scripts.\n",
    "    json_config = json.load(json_file)\n",
    "    # Copy another config since it will be modified in this code.\n",
    "    general_config = json_config.copy()\n",
    "\n",
    "# This notebook script will be converted to python script for other usage, the\n",
    "# following parts are intended not to be imported for other usage.\n",
    "if __name__ == \"__main__\":\n",
    "    # Settings for running in `slurm` or other platforms that are inconvenient\n",
    "    # to pass inputs directly.\n",
    "    if general_config[\"computing_platform\"] == \"slurm\":\n",
    "        # Pass arguments in shell script (e.g. python a.py --rnd_seed 0).\n",
    "        parser = argparse.ArgumentParser(description='argparse for slurm')\n",
    "        # Add `rnd_seed` argument to specify the random seed.\n",
    "        parser.add_argument('--rnd_seed', type=int, help='random seed')\n",
    "        parse_args = parser.parse_args()\n",
    "        general_config[\"rnd_seed\"] = parse_args.rnd_seed\n",
    "    else:\n",
    "        # Inputs for additional training information manually.\n",
    "        # Record current time.\n",
    "        general_config[\"ctime\"]  = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "        # Specifying a datetime if needed (to compatible with other exp results).\n",
    "        general_config[\"time\"]   = input(\"Datetime (default current time): \") \\\n",
    "            or general_config[\"ctime\"]\n",
    "        # This will recording the classical device for simulation (optional).\n",
    "        general_config[\"device\"] = input(\"Classical computing device     : \")\n",
    "        # The suffix of training result (optional).\n",
    "        general_config[\"suffix\"] = input(\"Suffix for this training exp   : \")\n",
    "\n",
    "    # Whether using Wandb to monitor the training procedure.\n",
    "    use_wandb = general_config[\"use_wandb\"]\n",
    "    if use_wandb:\n",
    "        import wandb\n",
    "        # Api can be used to fetch training results.\n",
    "        api = wandb.Api()\n",
    "\n",
    "    # Configuration of quantum devices (PennyLane simulation or IBMQ real devices).\n",
    "    quantum_config = {\"qdevice\": \"default.qubit\", \"qbackend\": \"\"}\n",
    "\n",
    "    # Test whether the code can run for at least 1 epoch.\n",
    "    if general_config[\"quick_test\"] == True:\n",
    "        general_config[\"max_epochs\"] = 1\n",
    "        general_config[\"num_bin_data\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical and Quantum Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical Message Passing Graph Neural Network (MPGNN)\n",
    "\n",
    "This classical model is built with GNN structure followed by a simple shallow fully connected linear network. The GNN is constructed with package `torch_geometric`, see [\"PyTorch Geometric\"](https://pytorch-geometric.readthedocs.io) for futher details. The tutorial for creating a `MessagePassing` class can be found at [\"Creating Message Passing Networks\"](https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_gnn.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessagePassing(MessagePassing):\n",
    "    def __init__(self, phi):\n",
    "        \"\"\"Undirected message passing model.\n",
    "        \n",
    "        Args:\n",
    "            phi : message passing function\n",
    "                See the paper or the \"Creating Message Passing Networks\"\n",
    "                for futher details.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(aggr=\"add\", flow=\"target_to_source\")\n",
    "        self.phi = phi\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        return self.propagate(edge_index, x=x)\n",
    "    \n",
    "    def message(self, x_i, x_j):\n",
    "        return self.phi(torch.cat((x_i, x_j), dim=-1))\n",
    "    \n",
    "    def update(self, aggr_out, x):\n",
    "        return aggr_out\n",
    "\n",
    "\n",
    "class GraphMPGNN(nn.Module):\n",
    "    def __init__(self, phi, mlp):\n",
    "        \"\"\"MPGNN with SUM as the aggregation function.\n",
    "        \n",
    "        Instead of combining this part with the class `ClassicalMPGNN` \n",
    "        below, we seperate out for the possibility of other design of \n",
    "        mlp.\n",
    "\n",
    "        Args:\n",
    "            phi : message passing function\n",
    "                See `MessagePassing` above.\n",
    "            mlp : Multi-layer perceptrons\n",
    "                Basically just a simple shallow fully connected linear\n",
    "                model for transforming dimensions.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.gnn = MessagePassing(phi)\n",
    "        self.mlp = mlp\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Graph neural network.\n",
    "        x = self.gnn(x, edge_index)\n",
    "\n",
    "        # Graph aggregation.\n",
    "        x = torch_geometric.nn.global_add_pool(x, batch)\n",
    "\n",
    "        # Shallow linear model.\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ClassicalMPGNN(GraphMPGNN):\n",
    "    def __init__(\n",
    "            self,\n",
    "            gnn_in: int,\n",
    "            gnn_out: int,\n",
    "            gnn_hidden: int,\n",
    "            gnn_layers: int,\n",
    "            mlp_hidden:int = 0,\n",
    "            mlp_layers:int = 0,\n",
    "            **kwargs\n",
    "    ):\n",
    "        \"\"\"Classical model for benchmarking\n",
    "\n",
    "        Arguments with prefix \"gnn\" are related to the message passing\n",
    "        function `phi`, which is constructed with a classical MLP.\n",
    "\n",
    "        Arguments with prefix \"mlp\" are related to the shallow linear\n",
    "        model after graph aggregation, which is also constructed with a \n",
    "        classical MLP.\n",
    "\n",
    "        The shallow linear model at the last step is designed for simply\n",
    "        transforming the dimension of GNN outputs to 1 dimension, then\n",
    "        the final 1 dimensional output represents the prediction of the \n",
    "        binary classification task, so the number of hidden neurons and\n",
    "        hidden layers are default as 0.\n",
    "        \n",
    "        Args:\n",
    "            gnn_in : int\n",
    "                The input channel dimension of `phi` in MPGNN.\n",
    "            gnn_out : int\n",
    "                The output channel dimension of `phi` in MPGNN.\n",
    "            gnn_hidden : int\n",
    "                Number of hidden neurons of `phi` in MPGNN.\n",
    "            gnn_layers : int\n",
    "                Number of hidden layers of `phi` in MPGNN.\n",
    "            mlp_hidden : int (default 0)\n",
    "                Number of hidden neurons of the shallow linear model.\n",
    "            mlp_layers : int (default 0)\n",
    "                Number of hidden layers of the shallow linear model.\n",
    "        \"\"\"\n",
    "        \n",
    "        # See `GraphMPGNN` above.\n",
    "        phi = module_model.ClassicalMLP(\n",
    "            in_channel=gnn_in,\n",
    "            out_channel=gnn_out,\n",
    "            hidden_channel=gnn_hidden,\n",
    "            num_layers=gnn_layers\n",
    "            )\n",
    "        mlp = module_model.ClassicalMLP(\n",
    "            in_channel=gnn_out,\n",
    "            out_channel=1,\n",
    "            hidden_channel=mlp_hidden,\n",
    "            num_layers=mlp_layers\n",
    "            )\n",
    "\n",
    "        super().__init__(phi, mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantum Complete Graph Neural Network (QCGNN)\n",
    "\n",
    "The main structure of QCGNN is written in the `module_model.py` script, and the following codes focus on how data is encoded to the quantum circuit (corresponding to the arguement `ctrl_enc` in `module_model.QCGNN_IX` or `module_model.QCGNN_0`). In this paper, we test the encoding ansatz constructed through angle encoding.\n",
    "\n",
    "Note that there are two encoding functions below (`pennylane_encoding` and `qiskit_encoding`), both are equivalent but run in different quantum devices.\n",
    "- `pennylane_encoding`: for simulation using PennyLane (such as \"default.qubit\").\n",
    "- `qiskit_encoding`: The multi-qubit gates in `pennylane_encoding` are decomposed to single-qubit and two-qubit gates. The decomposition method can be found at [Quantum Computation and Quantum Information](https://www.cambridge.org/highereducation/books/quantum-computation-and-quantum-information/01E10196D0A682A6AEFFEA52D53BE9AE#overview) section 4.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pennylane_encoding(ptc_input: torch.tensor, control_values: list[int], num_ir_qubits: int, num_nr_qubits: int):\n",
    "    \"\"\"Angle encoding ansatz for PennyLane\n",
    "\n",
    "    Args:\n",
    "        ptc_input : torch.tensor\n",
    "            One particle information of the jet.\n",
    "        control_values : list[int]\n",
    "            Control values of the multi-controlled gates.\n",
    "        num_ir_qubits : int\n",
    "            Number of IR qubits.\n",
    "        num_nr_qubits : int\n",
    "            Number of NR qubits.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check input shape due to version `pennylane==0.31.0` above.\n",
    "    if len(ptc_input.shape) > 1:\n",
    "        # The shape of `ptc_input` is (batch, 3)\n",
    "        theta, phi, omega = ptc_input[:,0], ptc_input[:,1], ptc_input[:,2]\n",
    "    else:\n",
    "        # The shape of `ptc_input` is (3,)\n",
    "        theta, phi, omega = ptc_input[0], ptc_input[1], ptc_input[2]\n",
    "    \n",
    "    # Encode data on NR qubits.\n",
    "    nr_wires = range(num_ir_qubits, num_ir_qubits + num_nr_qubits)\n",
    "    for wires in nr_wires:\n",
    "        # Add a Hadamard gate before the rotation gate.\n",
    "        ctrl_H = qml.ctrl(qml.Hadamard, control=range(num_ir_qubits), control_values=control_values)\n",
    "        ctrl_H(wires=wires)\n",
    "        # Appl general rotation gate.\n",
    "        ctrl_R = qml.ctrl(qml.Rot, control=range(num_ir_qubits), control_values=control_values)\n",
    "        ctrl_R(theta=theta, phi=phi, omega=omega, wires=wires)\n",
    "\n",
    "\n",
    "def qiskit_encoding(ptc_input: torch.tensor, control_values: list[int], num_ir_qubits: int, num_nr_qubits: int):\n",
    "    \"\"\"Angle encoding ansatz for IBMQ.\n",
    "\n",
    "    Args:\n",
    "        See `pennylane_encoding` above.\n",
    "    \"\"\"\n",
    "\n",
    "    # Decomposition of multi-qubit gates needs working qubits.\n",
    "    num_wk_qubits = num_ir_qubits - 1\n",
    "\n",
    "    # Check input shape due to version `pennylane==0.31.0` above.\n",
    "    if len(ptc_input.shape) > 1:\n",
    "        # The shape of `ptc_input` is (batch, 3)\n",
    "        theta, phi, omega = ptc_input[:,0], ptc_input[:,1], ptc_input[:,2]\n",
    "    else:\n",
    "        # The shape of `ptc_input` is (3,)\n",
    "        theta, phi, omega = ptc_input[0], ptc_input[1], ptc_input[2]\n",
    "    \n",
    "    # Target wires to be encoded.\n",
    "    nr_wires = range(num_ir_qubits + num_wk_qubits, \n",
    "                     num_ir_qubits + num_wk_qubits + num_nr_qubits)\n",
    "    \n",
    "    def control_condition_transform(control_values: list[int]):\n",
    "        \"\"\"Turn ctrl-0 to ctrl-1\n",
    "        \n",
    "        If control values == 0, use X-gate for transforming to 1.\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(len(control_values)):\n",
    "            # `i` also corresponds to the i-th qubit in IR.\n",
    "            bit = control_values[i]\n",
    "            if bit == 0:\n",
    "                qml.PauliX(wires=i)\n",
    "\n",
    "    def toffoli_tranformation(inverse: bool = False):\n",
    "        \"\"\"Decomposition of multi-contolled gates\n",
    "        \n",
    "        Use Toffoli transformation for decomposition of multi-controlled gates.\n",
    "\n",
    "        Args:\n",
    "            inverse : bool\n",
    "                Whether to apply inverse transformation or not.\n",
    "        \"\"\"\n",
    "\n",
    "        if (not inverse) and (num_ir_qubits > 1):\n",
    "            wk_qubit_t = num_ir_qubits # target qubit, also first working qubit\n",
    "            qml.Toffoli(wires=(0, 1, wk_qubit_t))\n",
    "        \n",
    "        if inverse:\n",
    "            toffoli_range = reversed(range(num_wk_qubits-1))\n",
    "        else:\n",
    "            toffoli_range = range(num_wk_qubits-1)\n",
    "        for i in toffoli_range:\n",
    "            ir_qubit_c = 2 + i # control qubit\n",
    "            wk_qubit_c = num_ir_qubits + i # control qubit\n",
    "            wk_qubit_t = num_ir_qubits + i + 1 # target qubit\n",
    "            qml.Toffoli(wires=(ir_qubit_c, wk_qubit_c, wk_qubit_t))\n",
    "\n",
    "        if inverse and (num_ir_qubits > 1):\n",
    "            wk_qubit_t = num_ir_qubits # target qubit, also first working qubit\n",
    "            qml.Toffoli(wires=(0, 1, wk_qubit_t))\n",
    "\n",
    "    # See \"Quantum Computation and Quantum Information\" section 4.3.\n",
    "    for wires in nr_wires:\n",
    "        control_condition_transform(control_values)\n",
    "        toffoli_tranformation()\n",
    "        \n",
    "        # The last working qubit becomes the control qubit.\n",
    "        wk_qubit_c = num_ir_qubits + num_wk_qubits - 1\n",
    "        # ctrl_H: decomposed by H = i Rx(pi) Ry(pi/2) up to a global phase.\n",
    "        qml.CRY(np.pi/2, wires=(wk_qubit_c, wires))\n",
    "        qml.CRX(np.pi, wires=(wk_qubit_c, wires))\n",
    "        # ctrl_R: Rot(phi, theta, omega) = Rz(omega) Ry(theta) Rz(phi).\n",
    "        qml.CRZ(phi, wires=(wk_qubit_c, wires))\n",
    "        qml.CRY(theta, wires=(wk_qubit_c, wires))\n",
    "        qml.CRZ(omega, wires=(wk_qubit_c, wires))\n",
    "        \n",
    "        toffoli_tranformation(inverse=True)\n",
    "        control_condition_transform(control_values)\n",
    "\n",
    "\n",
    "class QuantumRotQCGNN(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_ir_qubits: int,\n",
    "            num_nr_qubits: int,\n",
    "            num_layers: int,\n",
    "            num_reupload: int,\n",
    "            quantum_config: dict,\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Detemine the encoding method.\n",
    "        qdevice = quantum_config[\"qdevice\"] # quantum device\n",
    "        qbackend = quantum_config[\"qbackend\"] # quantum backend\n",
    "        if (\"qiskit\" in qdevice) or (\"qiskit\" in qbackend):\n",
    "            ctrl_enc = lambda ptc_input, control_values: \\\n",
    "                qiskit_encoding(ptc_input, control_values, num_ir_qubits, num_nr_qubits)\n",
    "        else:\n",
    "            ctrl_enc = lambda ptc_input, control_values: \\\n",
    "                pennylane_encoding(ptc_input, control_values, num_ir_qubits, num_nr_qubits)\n",
    "        self.ctrl_enc = ctrl_enc\n",
    "        \n",
    "        # Constructing `phi` and `mlp` just like MPGNN.\n",
    "        self.phi = QCGNN(\n",
    "            num_ir_qubits=num_ir_qubits,\n",
    "            num_nr_qubits=num_nr_qubits,\n",
    "            num_layers=num_layers,\n",
    "            num_reupload=num_reupload,\n",
    "            ctrl_enc=ctrl_enc,\n",
    "            **quantum_config\n",
    "            )\n",
    "        self.mlp = module_model.ClassicalMLP(\n",
    "            in_channel=num_nr_qubits,\n",
    "            out_channel=1,\n",
    "            hidden_channel=0,\n",
    "            num_layers=0\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # QCGNN.\n",
    "        x = self.phi(x)\n",
    "\n",
    "        # Shallow linear model.\n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical and quantum workflow\n",
    "\n",
    "- The main workflow is written in function `execute`, consists of\n",
    "  - Determine the name, id or other information for a training.\n",
    "  - Monitoring the training procedure, either with `csv` only or `wandb`.\n",
    "  - Record all the configurations (e.g. training, model, settings, .etc).\n",
    "  - Two modes can be specified, either `mode=\"train\"` or `mode=\"predict\"`\n",
    "    - `mode=\"train\"` will create a new training, with results saving at `result_dir` in `./config.json`.\n",
    "    - `mode=\"predict\"` will load `ckpt` files from `ckpt_dir` in `./config.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions will be used in `execute` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ckpt_path(ckpt_key: str):\n",
    "    \"\"\"Returns the ckpt path for the given key\n",
    "\n",
    "    Args:\n",
    "        ckpt_key : str\n",
    "            The key that helps finding the correct ckpt directory.\n",
    "    \"\"\"\n",
    "\n",
    "    # Find the correct ckpt file path.\n",
    "    pretrain_dir = general_config[\"pretrain_dir\"]\n",
    "    for dir_name in os.listdir(pretrain_dir):\n",
    "        rnd_seed = int(dir_name[-1])\n",
    "        if (ckpt_key in dir_name) and (rnd_seed == general_config[\"rnd_seed\"]):\n",
    "            ckpt_dir = os.path.join(pretrain_dir, dir_name, \"checkpoints\")\n",
    "            ckpt_file = os.listdir(ckpt_dir)[0]\n",
    "            ckpt_path = os.path.join(ckpt_dir, ckpt_file)\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(f\"ckpt NOT found in {pretrain_dir}: key = {ckpt_key}\")\n",
    "    \n",
    "    _log(f\"ckpt found at {ckpt_path}\")\n",
    "\n",
    "    return ckpt_path\n",
    "\n",
    "\n",
    "def generate_datamodule(\n",
    "        data_config: dict,\n",
    "        data_ratio: float,\n",
    "        batch_size: int,\n",
    "        graph: bool\n",
    "    ):\n",
    "    \"\"\"Generate datamodule for `execute` usage\n",
    "    \n",
    "    Args:\n",
    "        data_config : dict\n",
    "            See code below for detail usage.\n",
    "        data_ratio : float\n",
    "            Ratio of (# of training) / (# of training + testing).\n",
    "        batch_size : int\n",
    "            Number of data per batch.\n",
    "        graph : bool\n",
    "            Whether the dataset is generated in graph structure or not.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate uniform pt events.\n",
    "    fatjet_events = lambda channel: module_data.FatJetEvents(\n",
    "        channel=channel,\n",
    "        cut_pt=data_config[\"cut_pt\"],\n",
    "        subjet_radius=data_config[\"subjet_radius\"],\n",
    "        max_num_ptcs=data_config[\"max_num_ptcs\"],\n",
    "        pt_threshold=data_config[\"pt_threshold\"],\n",
    "    )\n",
    "    sig_fatjet_events = fatjet_events(data_config[\"sig\"])\n",
    "    bkg_fatjet_events = fatjet_events(data_config[\"bkg\"])\n",
    "    sig_events = sig_fatjet_events.generate_uniform_pt_events(\n",
    "        bin=data_config[\"bin\"], num_bin_data=data_config[\"num_bin_data\"])\n",
    "    bkg_events = bkg_fatjet_events.generate_uniform_pt_events(\n",
    "        bin=data_config[\"bin\"], num_bin_data=data_config[\"num_bin_data\"])\n",
    "\n",
    "    return module_data.JetDataModule(\n",
    "        sig_events=sig_events,\n",
    "        bkg_events=bkg_events,\n",
    "        data_ratio=data_ratio,\n",
    "        batch_size=batch_size,\n",
    "        graph=graph,\n",
    "        max_num_ptcs=data_config[\"max_num_ptcs\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### General `execute` function -> Main workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(\n",
    "        model: nn.Module,\n",
    "        general_config: dict,\n",
    "        model_config: dict,\n",
    "        data_config: dict,\n",
    "        graph: bool,\n",
    "        mode:str,\n",
    "        suffix:str = \"\"\n",
    "    ):\n",
    "    \"\"\"General training procedure.\n",
    "    \n",
    "    Workflow:\n",
    "    1. Initialize the training monitor.\n",
    "    2. Create data module and lightning model.\n",
    "    3. Train or predict, depending on `mode`.\n",
    "    4. Return training ID and summary.\n",
    "\n",
    "    Args:\n",
    "        model : nn.Module\n",
    "            PyTorch format model.\n",
    "        general_config : dict\n",
    "            Dictionary of general settings.\n",
    "        model_config : dict\n",
    "            Used for recording model information.\n",
    "        data_config : dict\n",
    "            Used for generating data module.\n",
    "        graph : bool\n",
    "            Whether the training dataset is in graph structure or not.\n",
    "        mode : str\n",
    "            Either \"train\" or \"predict\" only.\n",
    "        suffix : str\n",
    "            The training suffix.\n",
    "    \n",
    "    Returns:\n",
    "        1st argument : str\n",
    "            The training ID (can be used to restore wandb runs).\n",
    "        2nd argument : dict\n",
    "            Summary of the training result.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Seed everything.\n",
    "    L.seed_everything(general_config[\"rnd_seed\"])\n",
    "\n",
    "    # Record the total training time.\n",
    "    time_start = time.time()\n",
    "\n",
    "    # Use suffix for the training if needed.\n",
    "    if suffix != \"\":\n",
    "        suffix = \"_\" + suffix\n",
    "\n",
    "    # Additional model information (used for wandb filter).\n",
    "    model_config[\"model_name\"] = model.__class__.__name__\n",
    "    model_config[\"group_rnd\"] = f\"{model_config['base_model']}_{model_config['model_suffix']}-{data_config['data_suffix']}\"\n",
    "    model_config[\"group_rnd_full\"] = f\"{model_config['model_name']}-{model_config['group_rnd']}\"\n",
    "\n",
    "    # Monitor (either CSVLogger or WandbLogger) configuration and setup.\n",
    "    logger_config = {}\n",
    "    logger_config[\"project\"] = general_config[\"wandb_project\"]\n",
    "    logger_config[\"group\"] = f\"{data_config['sig']}_{data_config['bkg']}\"\n",
    "    logger_config[\"name\"] = f\"{model_config['group_rnd']}-{general_config['time']}_{general_config['device']}{suffix}_{general_config['rnd_seed']}\"\n",
    "    logger_config[\"id\"] = logger_config[\"name\"]\n",
    "    logger_config[\"save_dir\"] = general_config[\"result_dir\"]\n",
    "    if use_wandb:\n",
    "        logger = module_training.wandb_monitor(model, logger_config, general_config, model_config, data_config)\n",
    "    else:\n",
    "        logger = module_training.default_monitor(logger_config, general_config, model_config, data_config)\n",
    "\n",
    "    # Print out the training information.\n",
    "    print(\"\\n-------------------- Training information --------------------\")\n",
    "    print(\"| * general_config:\", general_config, \"\\n\")\n",
    "    print(\"| * data_config:\", data_config, \"\\n\")\n",
    "    print(\"| * model_config:\", model_config, \"\\n\")\n",
    "    print(\"| * logger_config:\", logger_config, \"\\n\")\n",
    "    print(\"--------------------------------------------------------------\\n\")\n",
    "    \n",
    "    # pytorch-lightning trainer.\n",
    "    trainer = L.Trainer(\n",
    "        logger = logger, \n",
    "        accelerator = general_config[\"accelerator\"],\n",
    "        max_epochs = general_config[\"max_epochs\"],\n",
    "        fast_dev_run = general_config[\"fast_dev_run\"],\n",
    "        log_every_n_steps = general_config[\"log_every_n_steps\"],\n",
    "        num_sanity_val_steps = 0,\n",
    "    )\n",
    "    \n",
    "    # pytorch-lightning data module.\n",
    "    data_module = generate_datamodule(\n",
    "        data_config=data_config, \n",
    "        data_ratio=general_config[\"data_ratio\"],\n",
    "        batch_size=general_config[\"batch_size\"],\n",
    "        graph=graph\n",
    "        )\n",
    "    \n",
    "    # pytorch-lightning model.\n",
    "    litmodel = module_training.BinaryLitModel(\n",
    "        model=model,\n",
    "        lr=model_config[\"lr\"],\n",
    "        graph=graph\n",
    "    )\n",
    "    \n",
    "    # Training or prediction.\n",
    "    if mode == \"train\":\n",
    "        trainer.fit(litmodel, datamodule=data_module)\n",
    "        train_summary = trainer.test(\n",
    "            model=litmodel,\n",
    "            dataloaders=data_module.train_dataloader()\n",
    "        )[0] # The output is like [summary_object], so use [0] to get the item.\n",
    "        test_summary = trainer.test(\n",
    "            model=litmodel,\n",
    "            dataloaders=data_module.test_dataloader()\n",
    "        )[0] # The output is like [summary_object], so use [0] to get the item.\n",
    "    elif mode == \"predict\":\n",
    "        # The ckpt key helped for finding correct checkpoints file.\n",
    "        data_suffix = (\n",
    "            f\"{data_config['abbrev']}_\"\n",
    "            f\"cut({data_config['cut_pt'][0]},{data_config['cut_pt'][1]})\"\n",
    "        )\n",
    "        model_name = model_config['model_name']\n",
    "        model_suffix = model_config['model_suffix']\n",
    "        ckpt_key = f\"{model_name}_{model_suffix}-{data_suffix}\"\n",
    "        # Train 8 particles only, and use the parameters to test other number \n",
    "        # of particles.\n",
    "        if model_config['model_name'] == QuantumRotQCGNN.__name__:\n",
    "            gnn_idx_qubits = model_config['gnn_idx_qubits']\n",
    "            ckpt_key = ckpt_key.replace(f\"qidx{gnn_idx_qubits}\", \"qidx3\")\n",
    "        # Get the correct checkpoints file path.\n",
    "        ckpt_path = get_ckpt_path(ckpt_key)\n",
    "        train_summary = trainer.test(\n",
    "            model=litmodel,\n",
    "            dataloaders=data_module.train_dataloader(),\n",
    "            ckpt_path=ckpt_path\n",
    "        )[0] # The output is like [summary_object], so use [0] to get the item.\n",
    "        test_summary = trainer.test(\n",
    "            model=litmodel,\n",
    "            dataloaders=data_module.test_dataloader(),\n",
    "            ckpt_path=ckpt_path\n",
    "        )[0] # The output is like [summary_object], so use [0] to get the item.\n",
    "\n",
    "    # Finish wandb monitoring if using WandbLogger.\n",
    "    if use_wandb:\n",
    "        wandb.finish()\n",
    "\n",
    "    # Summary of training results.\n",
    "    train_summary.update({\"data_mode\":\"train\"})\n",
    "    test_summary.update({\"data_mode\":\"test\"})\n",
    "    for _summary in [train_summary,test_summary]:\n",
    "        _summary.update(general_config)\n",
    "        _summary.update(data_config)\n",
    "        _summary.update(model_config)\n",
    "    summary = pd.DataFrame([train_summary, test_summary])\n",
    "    \n",
    "    # Calculating total training time.\n",
    "    time_end = time.time()\n",
    "    _log(f\"Time = {(time_end - time_start) / 60} minutes\")\n",
    "    print(\"\\n\", 100 * \"@\", \"\\n\")\n",
    "    \n",
    "    return logger_config[\"id\"], summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specific execution functions for MPGNN and QCGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_classical(\n",
    "        general_config: dict,\n",
    "        data_config: dict,\n",
    "        go: int,\n",
    "        gh: int,\n",
    "        gl: int,\n",
    "        lr: float,\n",
    "        mode: str\n",
    "    ):\n",
    "    \"\"\"Default setup for classical execution.\n",
    "    \n",
    "    Create with `ClassicalMPGNN` model.\n",
    "\n",
    "    Args:\n",
    "        general_config : dict\n",
    "            Dictionary of general settings.\n",
    "        data_config : dict\n",
    "            Data configuration for generating data module.\n",
    "        go : int\n",
    "            Dimension of GNN output channel.\n",
    "        gh : int\n",
    "            Number of hidden neurons in GNN.\n",
    "        gl : int\n",
    "            Number of hidden layers in GNN\n",
    "        lr : float\n",
    "            Learning rate.\n",
    "        mode : str\n",
    "            Either \"train\" for \"predict\".\n",
    "    \"\"\"\n",
    "    \n",
    "    # Suffix used for wandb filter and training result.\n",
    "    model_suffix = f\"go{go}_gh{gh}_gl{gl}_mh0_ml0\"\n",
    "    \n",
    "    # Configurations for constructing MPGNN.\n",
    "    model_config = {\n",
    "        \"gnn_in\": 6, \"gnn_out\": go, \"gnn_hidden\": gh, \"gnn_layers\": gl,\n",
    "        \"mlp_hidden\": 0, \"mlp_layers\": 0,\n",
    "        \"lr\": lr, \"model_suffix\": model_suffix,\n",
    "        \"base_model\": module_model.ClassicalMLP.__name__,\n",
    "    }\n",
    "    model = ClassicalMPGNN(**model_config)\n",
    "\n",
    "    # Execute training for classical models.\n",
    "    run_id, summary = execute(\n",
    "        model=model,\n",
    "        general_config=general_config,\n",
    "        model_config=model_config,\n",
    "        data_config=data_config,\n",
    "        graph=True,\n",
    "        mode=mode,\n",
    "        suffix=general_config[\"suffix\"],\n",
    "    )\n",
    "\n",
    "    # Check whether the model is trained successfully above threshold.\n",
    "    test_acc = summary[\"test_acc_epoch\"]\n",
    "    acc_threshold = general_config[\"retrain_threshold\"]\n",
    "    below_threshold = (test_acc < acc_threshold).any()\n",
    "    \n",
    "    # Too small model might not trainable, we retrain with random seed += 10.\n",
    "    if (mode == \"train\") and general_config[\"retrain\"] and below_threshold:\n",
    "        # Delete the failed training on Wandb.\n",
    "        if use_wandb:\n",
    "            account = general_config[\"wandb_account\"]\n",
    "            project = general_config[\"wandb_project\"]\n",
    "            run = api.run(f\"{account}/{project}/{run_id}\")\n",
    "            run.delete()\n",
    "        \n",
    "        # Increase random seed by 10, since we train 10 different seeds.\n",
    "        old_rnd_ssed = general_config[\"rnd_seed\"]\n",
    "        new_rnd_seed = old_rnd_ssed + general_config[\"retrain_cycle\"]\n",
    "        general_config[\"rnd_seed\"] = new_rnd_seed\n",
    "        _log(f\"Reinitialize model with new random seed = {new_rnd_seed}\")\n",
    "\n",
    "        # Retrain a new round.\n",
    "        run_id, summary = execute_classical(general_config, data_config,\n",
    "                                            go, gh, gl, lr, mode)\n",
    "    \n",
    "    return run_id, summary\n",
    "\n",
    "def execute_quantum(\n",
    "        general_config: dict,\n",
    "        data_config: dict,\n",
    "        qnn: int,\n",
    "        gl: int,\n",
    "        gr: int,\n",
    "        lr: float,\n",
    "        mode: str\n",
    "    ):\n",
    "    \"\"\"Default setup for quantum execution.\n",
    "    \n",
    "    Create with `QuantumRotQCGNN` model.\n",
    "\n",
    "    Args:\n",
    "        general_config : dict\n",
    "            Dictionary of general settings.\n",
    "        data_config : dict\n",
    "            Data configuration for generating data module.\n",
    "        qnn : int\n",
    "            Number of NR qubits.\n",
    "        gl : int\n",
    "            Number of strongly entangling layers of a VQC.\n",
    "        gr : int\n",
    "            Number of data reuploading times.\n",
    "        lr : float\n",
    "            Learning rate.\n",
    "        mode : str\n",
    "            Either \"train\" for \"predict\".\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of IR qubits\n",
    "    qidx = int(np.ceil(np.log2(data_config[\"max_num_ptcs\"])))\n",
    "\n",
    "    # Suffix used for wandb filter and training result.\n",
    "    model_suffix = f\"qidx{qidx}_qnn{qnn}_gl{gl}_gr{gr}\"\n",
    "\n",
    "    # Configurations for constructing QCGNN.\n",
    "    model_config = {\n",
    "        \"gnn_idx_qubits\": qidx,\n",
    "        \"gnn_nn_qubits\": qnn,\n",
    "        \"gnn_layers\": gl,\n",
    "        \"gnn_reupload\": gr,\n",
    "        \"lr\": lr,\n",
    "        \"model_suffix\": model_suffix,\n",
    "        \"base_model\": QCGNN.__name__,\n",
    "    }\n",
    "    model = QuantumRotQCGNN(\n",
    "        num_ir_qubits=qidx,\n",
    "        num_nr_qubits=qnn,\n",
    "        num_layers=gl,\n",
    "        num_reupload=gr,\n",
    "        quantum_config=quantum_config\n",
    "    )\n",
    "    \n",
    "    # Execute training for quantum models.\n",
    "    run_id, summary = execute(\n",
    "        model=model,\n",
    "        general_config=general_config,\n",
    "        model_config=model_config,\n",
    "        data_config=data_config,\n",
    "        graph=False,\n",
    "        mode=mode,\n",
    "        suffix=general_config[\"suffix\"],\n",
    "    )\n",
    "    \n",
    "    return run_id, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Prediction\n",
    "\n",
    "Jet dataset using 3 features $p_T$, $\\Delta\\eta$, $\\Delta\\phi$.\n",
    "- 2-prong v.s. 1-prong\n",
    "    - Signal: VzToZhToVevebb\n",
    "    - Background: VzToQCD\n",
    "- 3-prong v.s. 1-prong\n",
    "    - Signal: VzToTt\n",
    "    - Background: VzToQCD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_config(\n",
    "        sig: str,\n",
    "        bkg: str,\n",
    "        abbrev: str,\n",
    "        cut_pt: tuple[float, float],\n",
    "        bin: int,\n",
    "        subjet_radius: float,\n",
    "        num_bin_data: int,\n",
    "        max_num_ptcs: int,\n",
    "        pt_threshold: float,\n",
    "    ):\n",
    "    \"\"\"Generate a dictionary of data configurations\n",
    "    \n",
    "    For detail construction, see `module_data.py`.\n",
    "\n",
    "    Args:\n",
    "        sig : str\n",
    "            Signal channel (the directory name in `./jet_dataset`).\n",
    "        bkg : str\n",
    "            Background channel (the directory name in `./jet_dataset`).\n",
    "        abbrev : str\n",
    "            Abbreviation of the jet discrimination.\n",
    "        cut_pt : tuple[float, float]\n",
    "            Minimum and maximum range of jet pt.\n",
    "        subjet_radius : float\n",
    "            The radius for reclustering.\n",
    "        bin : int\n",
    "            How many bins that will uniformly distributed over cut_pt.\n",
    "        num_bin_data : int\n",
    "            Number of data that uniformly generated in each bin.\n",
    "        max_num_ptcs : int\n",
    "            Maximum number of particles in each jet.\n",
    "        pt_threshold : int\n",
    "            Ratio of particle pt / jet pt.\n",
    "\n",
    "    Returns:\n",
    "        dict : Dictionary of data configurations.\n",
    "    \"\"\"\n",
    "\n",
    "    data_config = {\n",
    "        \"sig\": sig,\n",
    "        \"bkg\": bkg,\n",
    "        \"abbrev\": abbrev,\n",
    "        \"cut_pt\": cut_pt,\n",
    "        \"subjet_radius\": subjet_radius,\n",
    "        \"bin\": bin,\n",
    "        \"num_bin_data\": num_bin_data,\n",
    "        \"max_num_ptcs\": max_num_ptcs,\n",
    "        \"pt_threshold\": pt_threshold,\n",
    "    }\n",
    "    data_config[\"data_suffix\"] = (\n",
    "        f\"{abbrev}_ptc{max_num_ptcs}_thres{pt_threshold}\"\n",
    "        f\"-nb{num_bin_data}_R{subjet_radius}\"\n",
    "    )\n",
    "\n",
    "    return data_config\n",
    "\n",
    "data_config_list = [\n",
    "    # 2-prong v.s. 1-prong.\n",
    "    generate_data_config(\n",
    "        sig=\"VzToZhToVevebb\", bkg=\"VzToQCD\", abbrev=\"BB-QCD\",\n",
    "        cut_pt=(800, 1000), subjet_radius=0, bin=10,\n",
    "        num_bin_data=general_config[\"num_bin_data\"],\n",
    "        max_num_ptcs=general_config[\"max_num_ptcs\"],\n",
    "        pt_threshold=general_config[\"pt_threshold\"],\n",
    "    ),\n",
    "    \n",
    "    # 3-prong v.s. 1-prong.\n",
    "    generate_data_config(\n",
    "        sig=\"VzToTt\", bkg=\"VzToQCD\", abbrev=\"TT-QCD\",\n",
    "        cut_pt=(800, 1000), subjet_radius=0, bin=10,\n",
    "        num_bin_data=general_config[\"num_bin_data\"],\n",
    "        max_num_ptcs=general_config[\"max_num_ptcs\"],\n",
    "        pt_threshold=general_config[\"pt_threshold\"],\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the model you want to train.\n",
    "for data_config, rnd_seed in product(data_config_list, range(3)):\n",
    "    general_config[\"rnd_seed\"] = rnd_seed\n",
    "    \n",
    "    # # Classical MPGNN with hidden neurons {3, 6, 9} and 2 layers.\n",
    "    # for g_dim in [3,6,9]:\n",
    "    #     execute_classical(general_config, data_config, go=g_dim, gh=g_dim, gl=2, lr=1E-3, mode=\"train\")\n",
    "\n",
    "    # # Best classical MPGNN with hidden neurons 1024 and 4 layers.\n",
    "    # execute_classical(general_config, data_config, go=1024, gh=1024, gl=4, lr=1E-3, mode=\"train\")\n",
    "\n",
    "    # # Quantum QCGNN with NR qubits = reuploads = {3, 6, 9}.\n",
    "    # for q in [3, 6, 9]:\n",
    "    #     execute_quantum(general_config, data_config, qnn=q, gl=1, gr=q, lr=1E-3, mode=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas data frame buffers for saving prediction values.\n",
    "c_df = pd.DataFrame()\n",
    "b_df = pd.DataFrame()\n",
    "q_df = pd.DataFrame()\n",
    "pred_dir = os.path.join(general_config[\"predictions_dir\"], \"ideal_model\")\n",
    "os.makedirs(pred_dir, exist_ok=True)\n",
    "\n",
    "num_ptcs_range = range(2, 16 + 1, 2)\n",
    "prediction_tuple = product(range(3), num_ptcs_range, data_config_list)\n",
    "\n",
    "# Uncomment the model you want to predict.\n",
    "for rnd_seed, max_num_ptcs, data_config in prediction_tuple:\n",
    "    data_config[\"max_num_ptcs\"] = max_num_ptcs\n",
    "    general_config[\"rnd_seed\"] = rnd_seed\n",
    "\n",
    "    # # Prediction for classical MPGNN.\n",
    "    # for gnn_dim in [3,6,9]:\n",
    "    #     # Get summary of prediction result.\n",
    "    #     _, summary = execute_classical(\n",
    "    #         general_config, data_config, go=gnn_dim, gh=gnn_dim, gl=2, lr=1E-3, mode=\"predict\")\n",
    "    #     # Concatenating to prediction buffers.\n",
    "    #     c_df = pd.concat((c_df, summary))\n",
    "    # # Saving prediction summary to csv file.\n",
    "    # csv_file = f\"classical-{general_config['num_bin_data']}_{rnd_seed}.csv\"\n",
    "    # c_df.to_csv(os.path.join(pred_dir, csv_file), index=False)\n",
    "\n",
    "    # # Prediction for best classical MPGNN.\n",
    "    # _, summary = execute_classical(\n",
    "    #     general_config, data_config, go=1024, gh=1024, gl=4, lr=1E-3, mode=\"predict\")\n",
    "    # b_df = pd.concat((b_df, summary))\n",
    "    # csv_file = f\"best_classical-{general_config['num_bin_data']}_{rnd_seed}.csv\"\n",
    "    # b_df.to_csv(os.path.join(pred_dir, csv_file), index=False)\n",
    "\n",
    "    # # Prediction for quantum QCGNN.\n",
    "    # for qnn_dim in [3,6,9]:\n",
    "    #     # Get summary of prediction result.\n",
    "    #     _, summary = execute_quantum(\n",
    "    #         general_config, data_config, qnn=qnn_dim, gl=1, gr=qnn_dim, lr=1E-3, mode=\"predict\")\n",
    "    #     # Concatenating to prediction buffers.\n",
    "    #     q_df = pd.concat((q_df, summary))\n",
    "    # # Saving prediction summary to csv file.\n",
    "    # csv_file = f\"quantum-{general_config['num_bin_data']}_{rnd_seed}.csv\"\n",
    "    # q_df.to_csv(os.path.join(pred_dir, csv_file), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "abdd0d95ca50f233d1202cce1ba28eab5ada50f7ec17823ef40ef9b79347f6f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
