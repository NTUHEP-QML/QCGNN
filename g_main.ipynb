{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic packages\n",
    "import pandas as pd\n",
    "import os, time, json, argparse\n",
    "from itertools import product\n",
    "\n",
    "# modules\n",
    "import module_data\n",
    "import module_model\n",
    "import module_training\n",
    "\n",
    "# qml tools\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "# ml tools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import lightning as L\n",
    "import torch_geometric.nn as geomodule_model\n",
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "# faster calculation on GPU but less precision\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.json\", \"r\") as json_file:\n",
    "    # read JSON config file\n",
    "    json_config = json.load(json_file)\n",
    "\n",
    "    # set random seed for reproducibility\n",
    "    L.seed_everything(json_config[\"rnd_seed\"])\n",
    "\n",
    "    # directory to store result\n",
    "    os.makedirs(json_config[\"result_dir\"], exist_ok=True)\n",
    "\n",
    "    # default configurations\n",
    "    config = json_config[\"config\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general configuration and setup\n",
    "config = json_config[\"config\"]\n",
    "config[\"ctime\"]  = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "config[\"time\"]   = input(\"Specify a datetime or leave empty (default current time): \") or config[\"ctime\"]\n",
    "config[\"device\"] = input(\"Enter the computing device (4090, slurm, node, etc.)    : \")\n",
    "config[\"suffix\"] = input(\"Suffix (the format looks like 'MODEL_SUFFIX_DATE')      : \")\n",
    "if config[\"device\"] == \"slurm\":\n",
    "    # pass arguments when running through slurm\n",
    "    parser = argparse.ArgumentParser(description='argparse for slurm')\n",
    "    parser.add_argument('--rnd_seed', type=int, help='random seed')\n",
    "    parse_args = parser.parse_args()\n",
    "    config[\"rnd_seed\"] = parse_args.rnd_seed\n",
    "\n",
    "# whether using wandb to monitor the training procedure\n",
    "use_wandb = json_config[\"use_wandb\"]\n",
    "if use_wandb:\n",
    "    import wandb\n",
    "    api = wandb.Api()\n",
    "\n",
    "# manual settings\n",
    "config[\"max_epochs\"]   = 1 # 30\n",
    "config[\"num_bin_data\"] = 10 # 500\n",
    "quantum_config         = json_config[\"pennylane_config\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical Message Passing Graph Neural Network (MPGNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessagePassing(MessagePassing):\n",
    "    def __init__(self, phi):\n",
    "        super().__init__(aggr=\"add\", flow=\"target_to_source\")\n",
    "        self.phi = phi\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.propagate(edge_index, x=x)\n",
    "    def message(self, x_i, x_j):\n",
    "        return self.phi(torch.cat((x_i, x_j), dim=-1))\n",
    "    def update(self, aggr_out, x):\n",
    "        return aggr_out\n",
    "\n",
    "class GraphMPGNN(nn.Module):\n",
    "    def __init__(self, phi, mlp):\n",
    "        super().__init__()\n",
    "        self.gnn = MessagePassing(phi)\n",
    "        self.mlp = mlp\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.gnn(x, edge_index)\n",
    "        x = geomodule_model.global_add_pool(x, batch)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "    \n",
    "class ClassicalMPGNN(GraphMPGNN):\n",
    "    def __init__(self, gnn_in, gnn_out, gnn_hidden, gnn_layers, mlp_hidden=0, mlp_layers=0, **kwargs):\n",
    "        phi = module_model.ClassicalMLP(in_channel=gnn_in, out_channel=gnn_out, hidden_channel=gnn_hidden, num_layers=gnn_layers)\n",
    "        mlp = module_model.ClassicalMLP(in_channel=gnn_out, out_channel=1, hidden_channel=mlp_hidden, num_layers=mlp_layers)\n",
    "        super().__init__(phi, mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantum Complete Graph Neural Network (QCGNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumRotQCGNN(nn.Module):\n",
    "    def __init__(self, num_ir_qubits, num_nr_qubits, num_layers, num_reupload, quantum_config):\n",
    "        super().__init__()\n",
    "        # rotation encoding on pennylane simulator\n",
    "        def pennylane_encoding(_input, control_values):\n",
    "            for i in range(num_nr_qubits):\n",
    "                ctrl_H = qml.ctrl(qml.Hadamard, control=range(num_ir_qubits), control_values=control_values)\n",
    "                ctrl_H(wires=num_ir_qubits+i)\n",
    "                ctrl_R = qml.ctrl(qml.Rot, control=range(num_ir_qubits), control_values=control_values)\n",
    "                ctrl_R(theta=_input[0], phi=_input[1], omega=_input[2], wires=num_ir_qubits+i)\n",
    "        \n",
    "        # rotation encoding on qiskit\n",
    "        num_wk_qubits = num_ir_qubits - 1\n",
    "        def qiskit_encoding(_input, control_values):\n",
    "            theta, phi, omega = _input[0], _input[1], _input[2]\n",
    "            # see N.C. page.184\n",
    "            for i in range(num_nr_qubits):\n",
    "                # control values\n",
    "                for q in range(num_ir_qubits):\n",
    "                    if control_values[q] == 0:\n",
    "                        qml.PauliX(wires=q)\n",
    "                # toffoli transformation\n",
    "                if num_ir_qubits >= 2:\n",
    "                    qml.Toffoli(wires=(0, 1, num_ir_qubits))\n",
    "                for q in range(num_wk_qubits-1):\n",
    "                    qml.Toffoli(wires=(2+q, num_ir_qubits+q, num_ir_qubits+1+q))\n",
    "                # ctrl_H: decomposed by H = i Rx(pi) Ry(pi/2) (if complete graph with power of 2 nodes -> relative phase i becomes global)\n",
    "                target_qubit = num_ir_qubits + num_wk_qubits + i\n",
    "                qml.CRY(np.pi/2, wires=(num_ir_qubits + num_wk_qubits - 1, target_qubit))\n",
    "                qml.CRX(np.pi, wires=(num_ir_qubits + num_wk_qubits - 1, target_qubit))\n",
    "                # ctrl_R: Rot(phi, theta, omega) = Rz(omega) Ry(theta) Rz(phi)\n",
    "                qml.CRZ(phi, wires=(num_ir_qubits + num_wk_qubits - 1, target_qubit))\n",
    "                qml.CRY(theta, wires=(num_ir_qubits + num_wk_qubits - 1, target_qubit))\n",
    "                qml.CRZ(omega, wires=(num_ir_qubits + num_wk_qubits - 1, target_qubit))\n",
    "                # toffoli inverse transformation\n",
    "                for q in reversed(range(num_wk_qubits-1)):\n",
    "                    qml.Toffoli(wires=(2+q, num_ir_qubits+q, num_ir_qubits+1+q))\n",
    "                if num_ir_qubits >= 2:\n",
    "                    qml.Toffoli(wires=(0, 1, num_ir_qubits))\n",
    "                # control values\n",
    "                for q in range(num_ir_qubits):\n",
    "                    if control_values[q] == 0:\n",
    "                        qml.PauliX(wires=q)\n",
    "\n",
    "        # constructing QCGNN like a MPGNN\n",
    "        if \"qiskit\" in quantum_config[\"qdevice\"]:\n",
    "            self.phi = module_model.QCGNN(num_ir_qubits, num_nr_qubits, num_layers, num_reupload, ctrl_enc=qiskit_encoding, **quantum_config)\n",
    "        else:\n",
    "            self.phi = module_model.QCGNN(num_ir_qubits, num_nr_qubits, num_layers, num_reupload, ctrl_enc=pennylane_encoding, **quantum_config)\n",
    "        self.mlp = module_model.ClassicalMLP(in_channel=num_nr_qubits, out_channel=1, hidden_channel=0, num_layers=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # inputs should be 1-dim for each data, otherwise it would be confused with batch shape\n",
    "        x = torch.flatten(x, start_dim=-2, end_dim=-1)\n",
    "        x = self.phi(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(model, model_config, data_module, data_config, graph, mode, suffix=\"\"):\n",
    "    time_start = time.time()\n",
    "    if suffix != \"\":\n",
    "        suffix = \"_\" + suffix\n",
    "\n",
    "    # use wandb monitoring if needed\n",
    "    logger_config = {}\n",
    "    logger_config[\"project\"]    = json_config[\"project\"]\n",
    "    logger_config[\"group\"]      = f\"{data_config['sig']}_{data_config['bkg']}\"\n",
    "    logger_config[\"model_name\"] = model.__class__.__name__\n",
    "    logger_config[\"group_rnd\"]  = f\"{model.__class__.__name__}_{model_config['model_suffix']} | {data_config['data_suffix']}\"\n",
    "    if \"qiskit\" in quantum_config[\"qdevice\"]:\n",
    "        logger_config[\"name\"] = f\"{logger_config['group_rnd']} | {config['time']}_{quantum_config['qdevice']}{suffix}_{config['rnd_seed']}\"\n",
    "    else:\n",
    "        logger_config[\"name\"] = f\"{logger_config['group_rnd']} | {config['time']}_{config['device']}{suffix}_{config['rnd_seed']}\"\n",
    "    logger_config[\"id\"]       = logger_config[\"name\"]\n",
    "    logger_config[\"save_dir\"] = json_config[\"result_dir\"]\n",
    "    if use_wandb:\n",
    "        logger = module_training.wandb_monitor(model, logger_config, config, model_config, data_config)\n",
    "    else:\n",
    "        logger = module_training.default_monitor(logger_config, config, model_config, data_config)\n",
    "\n",
    "    # training information\n",
    "    print(\"\\n-------------------- Training information --------------------\")\n",
    "    print(\"| * config:\", config)\n",
    "    print(\"| * data_config:\", data_config)\n",
    "    print(\"| * model_config:\", model_config)\n",
    "    print(\"| * logger_config:\", logger_config)\n",
    "    print(\"--------------------------------------------------------------\\n\")\n",
    "    \n",
    "    # pytorch lightning setup\n",
    "    trainer = L.Trainer(\n",
    "        logger               = logger, \n",
    "        accelerator          = config[\"accelerator\"],\n",
    "        max_epochs           = config[\"max_epochs\"],\n",
    "        fast_dev_run         = config[\"fast_dev_run\"],\n",
    "        log_every_n_steps    = config[\"log_every_n_steps\"],\n",
    "        num_sanity_val_steps = 0,\n",
    "        )\n",
    "    litmodel = module_training.BinaryLitModel(model, lr=model_config[\"lr\"], graph=graph)\n",
    "    \n",
    "    if mode == \"train\":\n",
    "        trainer.fit(litmodel, datamodule=data_module)\n",
    "        train_summary = trainer.test(litmodel, dataloaders=data_module.train_dataloader())[0]\n",
    "        test_summary  = trainer.test(litmodel, dataloaders=data_module.test_dataloader())[0]\n",
    "    elif mode == \"predict\":\n",
    "        ckpt_dir = json_config[\"ckpt_dir\"]\n",
    "        ckpt_key = f\"{logger_config['model_name']}_{model_config['model_suffix']}\"\n",
    "        for f in os.listdir(ckpt_dir):\n",
    "            if logger_config['model_name'] == QuantumRotQCGNN.__name__:\n",
    "                ckpt_key = ckpt_key.replace(f\"qidx{model_config['gnn_idx_qubits']}\", \"qidx3\")\n",
    "            if ckpt_key in f and int(f[-1]) == config[\"rnd_seed\"]:\n",
    "                ckpt_path = os.path.join(ckpt_dir, f, \"checkpoints\")\n",
    "                ckpt_path = os.path.join(ckpt_path, os.listdir(ckpt_path)[0])\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(f\"# ModelLog: ckpt NOT found in {ckpt_dir} -> {ckpt_key}\")\n",
    "        print(f\"# ModelLog: ckpt found at {ckpt_path}\")\n",
    "        train_summary = trainer.test(litmodel, dataloaders=data_module.train_dataloader(), ckpt_path=ckpt_path)[0]\n",
    "        test_summary  = trainer.test(litmodel, dataloaders=data_module.test_dataloader(), ckpt_path=ckpt_path)[0]\n",
    "\n",
    "    # finish wandb monitoring\n",
    "    if use_wandb:\n",
    "        wandb.finish()\n",
    "\n",
    "    # summary\n",
    "    train_summary.update({\"data_mode\":\"train\"})\n",
    "    test_summary.update({\"data_mode\":\"test\"})\n",
    "    for _summary in [train_summary,test_summary]:\n",
    "        _summary.update(config)\n",
    "        _summary.update(data_config)\n",
    "        _summary.update(model_config)\n",
    "    summary = pd.DataFrame([train_summary, test_summary])\n",
    "\n",
    "    time_end = time.time()\n",
    "    print(f\"# ModelLog: Time = {(time_end - time_start) / 60} minutes\")\n",
    "    print(\"\\n\", 100 * \"@\", \"\\n\")\n",
    "    return logger_config[\"id\"], summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datamodule(data_config, graph):\n",
    "    sig_fatjet_events = module_data.FatJetEvents(channel=data_config[\"sig\"], cut_pt=data_config[\"cut\"], subjet_radius=data_config[\"subjet_radius\"], num_pt_ptcs=data_config[\"num_pt_ptcs\"])\n",
    "    bkg_fatjet_events = module_data.FatJetEvents(channel=data_config[\"bkg\"], cut_pt=data_config[\"cut\"], subjet_radius=data_config[\"subjet_radius\"], num_pt_ptcs=data_config[\"num_pt_ptcs\"])\n",
    "    sig_events  = sig_fatjet_events.generate_uniform_pt_events(bin=data_config[\"bin\"], num_bin_data=data_config[\"num_bin_data\"])\n",
    "    bkg_events  = bkg_fatjet_events.generate_uniform_pt_events(bin=data_config[\"bin\"], num_bin_data=data_config[\"num_bin_data\"])\n",
    "    data_suffix = f\"{data_config['abbrev']}_cut{data_config['cut']}_ptc{data_config['num_pt_ptcs']}_bin{data_config['bin']}-{data_config['num_bin_data']}_R{data_config['subjet_radius']}\"\n",
    "    data_config[\"data_suffix\"] = data_suffix\n",
    "    return module_data.JetDataModule(sig_events, bkg_events, data_ratio=config[\"data_ratio\"], batch_size=config[\"batch_size\"], graph=graph)\n",
    "\n",
    "def execute_classical(data_config, go, gh, gl, lr, mode):\n",
    "    '''\n",
    "        go -> dimension of gnn output\n",
    "        gh -> dimension of gnn hidden neurons\n",
    "        gl -> number of gnn hidden layers\n",
    "    '''\n",
    "    data_module  = generate_datamodule(data_config, graph=True)\n",
    "    model_suffix = f\"go{go}_gh{gh}_gl{gl}_mh0_ml0\"\n",
    "    model_config = {\"gnn_in\":6, \"gnn_out\":go, \"gnn_hidden\":gh, \"gnn_layers\":gl, \"mlp_hidden\":0, \"mlp_layers\":0, \"lr\":lr, \"model_suffix\":model_suffix}\n",
    "    model        = ClassicalMPGNN(**model_config)\n",
    "    run_id, summary = execute(model, model_config, data_module, data_config, graph=True, mode=mode, suffix=config[\"suffix\"])\n",
    "    if mode == \"train\":\n",
    "        while (summary[\"test_acc_epoch\"] < json_config[\"retrain_threshold\"]).any() and json_config[\"retrain\"]:\n",
    "            if use_wandb:\n",
    "                run = api.run(f\"{json_config['wandb_id']}/{json_config['project']}/{run_id}\") if use_wandb else None\n",
    "                run.delete()\n",
    "            config[\"rnd_seed\"] += json_config[\"retrain_cycle\"]\n",
    "            L.seed_everything(config[\"rnd_seed\"])\n",
    "            print(f\"\\n # ModelLog: Reinitialize model with new rnd_seed = {config['rnd_seed']}\\n\")\n",
    "            model = ClassicalMPGNN(**model_config)\n",
    "            run_id, summary = execute(model, model_config, data_module, data_config, graph=True, mode=mode, suffix=config[\"suffix\"])\n",
    "    return run_id, summary\n",
    "\n",
    "def execute_quantum(data_config, qnn, gl, gr, lr, mode):\n",
    "    '''\n",
    "        qnn -> number of NR qubits\n",
    "        gl  -> number of strongly entangling layers\n",
    "        gr  -> number of data reuploading\n",
    "    '''\n",
    "    data_module     = generate_datamodule(data_config, graph=False)\n",
    "    qidx            = int(np.ceil(np.log2(data_config[\"num_pt_ptcs\"])))\n",
    "    model_suffix    = f\"qidx{qidx}_qnn{qnn}_gl{gl}_gr{gr}\"\n",
    "    model_config    = {\"gnn_idx_qubits\":qidx, \"gnn_nn_qubits\":qnn, \"gnn_layers\":gl, \"gnn_reupload\":gr, \"lr\":lr, \"model_suffix\":model_suffix}\n",
    "    model           = QuantumRotQCGNN(num_ir_qubits=qidx, num_nr_qubits=qnn, num_layers=gl, num_reupload=gr, quantum_config=quantum_config)\n",
    "    run_id, summary = execute(model, model_config, data_module, data_config, graph=False, mode=mode, suffix=config[\"suffix\"])\n",
    "    return run_id, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {\"sig\": \"VzToZhToVevebb\", \"bkg\": \"VzToQCD\", \"abbrev\":\"BB-QCD\", \"cut\": (800, 1000), \"bin\":10, \"subjet_radius\":0, \"num_bin_data\":config[\"num_bin_data\"], \"num_pt_ptcs\":8}\n",
    "# data_config = {\"sig\": \"VzToTt\", \"bkg\": \"VzToQCD\", \"abbrev\":\"TT-QCD\", \"cut\": (800, 1000), \"bin\":10, \"subjet_radius\":0, \"num_bin_data\":config[\"num_bin_data\"], \"num_pt_ptcs\":8}\n",
    "\n",
    "# training\n",
    "for rnd_seed in range(1):\n",
    "    config[\"rnd_seed\"] = rnd_seed\n",
    "    L.seed_everything(config[\"rnd_seed\"])\n",
    "    \n",
    "    # # classical\n",
    "    # for g_dim in [3,6,9]:\n",
    "    #     execute_classical(data_config, go=g_dim, gh=g_dim, gl=2, lr=1E-3, mode=\"train\")\n",
    "\n",
    "    # # best classical\n",
    "    # execute_classical(data_config, go=1024, gh=1024, gl=4, lr=1E-3, mode=\"train\")\n",
    "\n",
    "    # # quantum\n",
    "    # for q in [3,6,9]:\n",
    "    #     if q == 3:\n",
    "    #         lr = 1E-2\n",
    "    #     else:\n",
    "    #         lr = 1E-3\n",
    "    #     execute_quantum(data_config, qnn=q, gl=1, gr=q, lr=lr, mode=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_configs = [\n",
    "    {\"sig\": \"VzToZhToVevebb\", \"bkg\": \"VzToQCD\", \"abbrev\":\"BB-QCD\", \"cut\": (800, 1000), \"bin\":10, \"subjet_radius\":0, \"num_bin_data\":config[\"num_bin_data\"], \"num_pt_ptcs\":None},\n",
    "    {\"sig\": \"VzToTt\", \"bkg\": \"VzToQCD\", \"abbrev\":\"TT-QCD\", \"cut\": (800, 1000), \"bin\":10, \"subjet_radius\":0, \"num_bin_data\":config[\"num_bin_data\"], \"num_pt_ptcs\":None},\n",
    "]\n",
    "\n",
    "# c_df, b_df, q_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "# num_ptcs_range = range(2, 16+1, 2)\n",
    "# for rnd_seed, num_pt_ptcs in product(range(10), num_ptcs_range):\n",
    "#     for data_config in data_configs:\n",
    "#         data_config[\"num_pt_ptcs\"] = num_pt_ptcs\n",
    "#         config[\"rnd_seed\"] = rnd_seed\n",
    "#         L.seed_everything(config[\"rnd_seed\"])\n",
    "\n",
    "        # # classical\n",
    "        # for g_dim in [3,6,9]:\n",
    "        #     _, summary = execute_classical(data_config, go=g_dim, gh=g_dim, gl=2, l=1E-3, mode=\"predict\")\n",
    "        #     c_df = pd.concat((c_df, summary))\n",
    "        #     c_df.to_csv(f\"csv/classical-{config['num_bin_data']}_{rnd_seed}.csv\", index=False)\n",
    "\n",
    "        # # best classical\n",
    "        # _, summary = execute_classical(data_config, go=1024, gh=1024, gl=4, l=1E-3, mode=\"predict\")\n",
    "        # b_df = pd.concat((b_df, summary))\n",
    "        # b_df.to_csv(f\"csv/classical-{config['num_bin_data']}_best.csv\", index=False)\n",
    "\n",
    "        # # quantum\n",
    "        # for q in [3,6,9]:\n",
    "        #     _, summary = execute_quantum(data_config, qnn=q, gl=1, gr=q, mode=\"predict\")\n",
    "        #     q_df = pd.concat((c_df, summary))\n",
    "        #     q_df.to_csv(f\"csv/qnn{q}_gl1_gr{q}_ptc({num_ptcs_range[0]},{num_ptcs_range[-1]})-{config['num_bin_data']}_{rnd_seed}.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "abdd0d95ca50f233d1202cce1ba28eab5ada50f7ec17823ef40ef9b79347f6f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
