{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic packages\n",
    "import os, time, sys, argparse\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# model template\n",
    "import module_model\n",
    "import module_training\n",
    "\n",
    "# data\n",
    "import module_data\n",
    "import awkward as ak\n",
    "\n",
    "# qml\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "\n",
    "# pytorch_lightning\n",
    "import lightning as L\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "# pytorch_geometric\n",
    "import torch_geometric.nn as geomodule_model\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "# reproducibility\n",
    "L.seed_everything(3020616)\n",
    "\n",
    "# faster calculation on GPU but less precision\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "# directory for saving results\n",
    "result_dir = \"./result\"\n",
    "os.makedirs(result_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global settings\n",
    "config = {}\n",
    "config[\"wandb\"]    = False # <-----------------------------------------------\n",
    "config[\"time\"]     = input(\"Specify a datetime or leave empty (default current time): \") or time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "config[\"device\"]   = input(\"Enter the computing device (4090, slurm, node, etc.)    : \")\n",
    "config[\"project\"]  = \"g_main\"\n",
    "config[\"rnd_seed\"] = int(input(\"Set random seed = \"))\n",
    "\n",
    "# # parser (if needed)\n",
    "# parser = argparse.ArgumentParser(description='argparse for slurm')\n",
    "# parser.add_argument('--rnd_seed', type=int, help='random seed')\n",
    "# parse_args = parser.parse_args()\n",
    "# config[\"rnd_seed\"] = parse_args.rnd_seed\n",
    "\n",
    "# training configuration\n",
    "config[\"num_train_ratio\"]   = 0.8\n",
    "config[\"num_bin_data\"]      = 500\n",
    "config[\"batch_size\"]        = 100\n",
    "config[\"num_workers\"]       = 0\n",
    "config[\"max_epochs\"]        = 30 # <-----------------------------------------------\n",
    "config[\"accelerator\"]       = \"cpu\"\n",
    "config[\"fast_dev_run\"]      = False\n",
    "config[\"log_every_n_steps\"] = config[\"batch_size\"] // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JetDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, sig_events, bkg_events, graph=True):\n",
    "        super().__init__()\n",
    "        # whether transform to torch_geometric graph data\n",
    "        self.graph = graph\n",
    "\n",
    "        # jet events\n",
    "        self.max_num_ptcs = max(\n",
    "            max(ak.count(sig_events[\"fast_pt\"], axis=1)),\n",
    "            max(ak.count(bkg_events[\"fast_pt\"], axis=1)))\n",
    "        sig_events = self._preprocess(sig_events)\n",
    "        bkg_events = self._preprocess(bkg_events)\n",
    "        print(f\"\\nDataLog: Max number of particles = {self.max_num_ptcs}\\n\")\n",
    "\n",
    "        # prepare dataset for dataloader\n",
    "        train_idx = int(config[\"num_train_ratio\"] * len(sig_events))\n",
    "        self.train_dataset = self._dataset(sig_events[:train_idx], 1) + self._dataset(bkg_events[:train_idx], 0)\n",
    "        self.test_dataset  = self._dataset(sig_events[train_idx:], 1) + self._dataset(bkg_events[train_idx:], 0)\n",
    "\n",
    "    def _preprocess(self, events):\n",
    "        # \"_\" prefix means that it is a fastjet feature\n",
    "        fatjet_radius = 0.8\n",
    "        f1 = np.arctan(events[\"fast_pt\"] / events[\"fatjet_pt\"])\n",
    "        f2 = events[\"fast_delta_eta\"] / fatjet_radius * (np.pi/2)\n",
    "        f3 = events[\"fast_delta_phi\"] / fatjet_radius * (np.pi/2)\n",
    "        arrays = ak.zip([f1, f2, f3])\n",
    "        arrays = arrays.to_list()\n",
    "        events = [torch.tensor(arrays[i], dtype=torch.float32, requires_grad=False) for i in range(len(arrays))]\n",
    "        return events\n",
    "\n",
    "    def _dataset(self, events, y):\n",
    "        if self.graph == True:\n",
    "            # create pytorch_geometric \"Data\" object\n",
    "            dataset = []\n",
    "            for i in range(len(events)):\n",
    "                x = events[i]\n",
    "                edge_index = list(product(range(len(x)), range(len(x))))\n",
    "                edge_index = torch.tensor(edge_index, requires_grad=False).transpose(0, 1)\n",
    "                dataset.append(Data(x=x, edge_index=edge_index, y=y))\n",
    "        else:\n",
    "            pad     = lambda x: torch.nn.functional.pad(x, (0,0,0,self.max_num_ptcs-len(x)), mode=\"constant\", value=0)\n",
    "            dataset = module_training.TorchDataset(x=[pad(events[i]) for i in range(len(events))], y=[y]*len(events))\n",
    "        return dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        if self.graph == True:\n",
    "            return GeoDataLoader(self.train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "        else:\n",
    "            return TorchDataLoader(self.train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if self.graph == True:\n",
    "            return GeoDataLoader(self.test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "        else:\n",
    "            return TorchDataLoader(self.test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        if self.graph == True:\n",
    "            return GeoDataLoader(self.test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "        else:\n",
    "            return TorchDataLoader(self.test_dataset, batch_size=config[\"batch_size\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessagePassing(MessagePassing):\n",
    "    def __init__(self, phi):\n",
    "        super().__init__(aggr=\"add\", flow=\"target_to_source\")\n",
    "        self.phi = phi\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.propagate(edge_index, x=x)\n",
    "    def message(self, x_i, x_j):\n",
    "        return self.phi(torch.cat((x_i, x_j), dim=-1))\n",
    "    def update(self, aggr_out, x):\n",
    "        return aggr_out\n",
    "\n",
    "class Graph2PCGNN(nn.Module):\n",
    "    def __init__(self, phi, mlp):\n",
    "        super().__init__()\n",
    "        self.gnn = MessagePassing(phi)\n",
    "        self.mlp = mlp\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.gnn(x, edge_index)\n",
    "        x = geomodule_model.global_add_pool(x, batch)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "    \n",
    "class Classical2PCGNN(Graph2PCGNN):\n",
    "    def __init__(self, gnn_in, gnn_out, gnn_hidden, gnn_layers, mlp_hidden=0, mlp_layers=0, **kwargs):\n",
    "        phi = module_model.ClassicalMLP(in_channel=gnn_in, out_channel=gnn_out, hidden_channel=gnn_hidden, num_layers=gnn_layers)\n",
    "        mlp = module_model.ClassicalMLP(in_channel=gnn_out, out_channel=1, hidden_channel=mlp_hidden, num_layers=mlp_layers)\n",
    "        super().__init__(phi, mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantum Complete Graph Neural Network (QCGNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumRotFCGNN(nn.Module):\n",
    "    def __init__(self, num_ir_qubits, num_nr_qubits, num_layers, num_reupload, **kwargs):\n",
    "        super().__init__()\n",
    "        # rotation encoding\n",
    "        def ctrl_rot_encoding(_input, control, control_values):\n",
    "            for i in range(num_nr_qubits):\n",
    "                ctrl_H = qml.ctrl(qml.Hadamard, control=control, control_values=control_values)\n",
    "                ctrl_H(wires=num_ir_qubits+i)\n",
    "                ctrl_R = qml.ctrl(qml.Rot, control=control, control_values=control_values)\n",
    "                ctrl_R(theta=_input[0], phi=_input[1], omega=_input[2], wires=num_ir_qubits+i)\n",
    "        \n",
    "        # constructing QCGNN like a MPGNN\n",
    "        self.phi = module_model.QCGNN(num_ir_qubits, num_nr_qubits, num_layers, num_reupload, ctrl_enc_gate=ctrl_rot_encoding)\n",
    "        self.mlp = module_model.ClassicalMLP(in_channel=num_nr_qubits, out_channel=1, hidden_channel=0, num_layers=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # inputs should be 1-dim for each data, otherwise it would be confused with batch shape\n",
    "        x = torch.flatten(x, start_dim=-2, end_dim=-1)\n",
    "        x = self.phi(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, model_config, data_module, data_config, graph, suffix=\"\"):\n",
    "    # use wandb monitoring if needed\n",
    "    if config[\"wandb\"] == True:\n",
    "        model_config[\"model_name\"] = model.__class__.__name__\n",
    "        model_config[\"group_rnd\"]  = f\"{model_config['model_name']}_{model_config['model_suffix']} | {data_config['data_suffix']}\"\n",
    "        logger_config = {}\n",
    "        logger_config[\"project\"]  = config[\"project\"]\n",
    "        logger_config[\"group\"]    = f\"{data_config['sig']}_{data_config['bkg']}\"\n",
    "        logger_config[\"name\"]     = f\"{model_config['group_rnd']} | {config['time']}_{config['device']}_{config['rnd_seed']} {suffix}\"\n",
    "        logger_config[\"id\"]       = logger_config[\"name\"]\n",
    "        logger_config[\"save_dir\"] = result_dir\n",
    "        logger = module_training.wandb_monitor(model, logger_config, config, model_config)\n",
    "    else:\n",
    "        logger = None\n",
    "\n",
    "    # training information\n",
    "    print(\"-------------------- Training information --------------------\\n\")\n",
    "    print(\"config:\", config, \"\")\n",
    "    print(\"data_config:\", data_config, \"\")\n",
    "    print(\"model_config:\", model_config, \"\")\n",
    "    if config[\"wandb\"] == True:\n",
    "        print(\"logger_config:\", logger_config, \"\")\n",
    "    print(\"--------------------------------------------------------------\\n\")\n",
    "    \n",
    "    # pytorch lightning setup\n",
    "    trainer = L.Trainer(\n",
    "        logger               = logger, \n",
    "        accelerator          = config[\"accelerator\"],\n",
    "        max_epochs           = config[\"max_epochs\"],\n",
    "        fast_dev_run         = config[\"fast_dev_run\"],\n",
    "        log_every_n_steps    = config[\"log_every_n_steps\"],\n",
    "        num_sanity_val_steps = 0,\n",
    "        )\n",
    "    litmodel = module_training.BinaryLitModel(model, lr=model_config[\"lr\"], graph=graph)\n",
    "    trainer.fit(litmodel, datamodule=data_module)\n",
    "    trainer.test(litmodel, datamodule=data_module)\n",
    "\n",
    "    # finish wandb monitoring\n",
    "    if config[\"wandb\"] == True:\n",
    "        module_training.wandb_finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_config = {\"sig\": \"VzToZhToVevebb\", \"bkg\": \"VzToQCD\", \"abbrev\":\"BB-QCD\", \"cut\": (800, 1000), \"bin\":10, \"subjet_radius\":0, \"num_bin_data\":config[\"num_bin_data\"], \"num_pt_ptcs\":8}\n",
    "data_config = {\"sig\": \"VzToTt\", \"bkg\": \"VzToQCD\", \"abbrev\":\"TT-QCD\", \"cut\": (800, 1000), \"bin\":10, \"subjet_radius\":0, \"num_bin_data\":config[\"num_bin_data\"], \"num_pt_ptcs\":8}\n",
    "\n",
    "sig_fatjet_events = module_data.FatJetEvents(channel=data_config[\"sig\"], cut_pt=data_config[\"cut\"], subjet_radius=data_config[\"subjet_radius\"], num_pt_ptcs=data_config[\"num_pt_ptcs\"])\n",
    "bkg_fatjet_events = module_data.FatJetEvents(channel=data_config[\"bkg\"], cut_pt=data_config[\"cut\"], subjet_radius=data_config[\"subjet_radius\"], num_pt_ptcs=data_config[\"num_pt_ptcs\"])\n",
    "\n",
    "L.seed_everything(config[\"rnd_seed\"])\n",
    "sig_events  = sig_fatjet_events.generate_uniform_pt_events(bin=data_config[\"bin\"], num_bin_data=data_config[\"num_bin_data\"])\n",
    "bkg_events  = bkg_fatjet_events.generate_uniform_pt_events(bin=data_config[\"bin\"], num_bin_data=data_config[\"num_bin_data\"])\n",
    "data_suffix = f\"{data_config['abbrev']}_cut{data_config['cut']}_ptc{data_config['num_pt_ptcs']}_bin{data_config['bin']}-{data_config['num_bin_data']}_R{data_config['subjet_radius']}\"\n",
    "data_config[\"data_suffix\"] = data_suffix\n",
    "\n",
    "# # classical ML\n",
    "# data_module  = JetDataModule(sig_events, bkg_events, graph=True)\n",
    "# go, gh, gl   = 3, 3, 3\n",
    "# mh, ml       = 0, 0\n",
    "# model_suffix = f\"go{go}_gh{gh}_gl{gl}_mh{mh}_ml{ml}\"\n",
    "# model_config = {\"gnn_in\":6, \"gnn_out\":go, \"gnn_hidden\":gh, \"gnn_layers\":gl, \"mlp_hidden\":mh, \"mlp_layers\":ml, \"lr\":1E-3, \"model_suffix\":model_suffix}\n",
    "# model        = Classical2PCGNN(**model_config) \n",
    "# train(model, model_config, data_module, data_config, graph=True, suffix=\"\")\n",
    "\n",
    "# # QFCGNN\n",
    "# data_module   = JetDataModule(sig_events, bkg_events, graph=False)\n",
    "# qir, qnr      = int(np.ceil(np.log2(data_config[\"num_pt_ptcs\"]))), 2\n",
    "# nl, nr        = 1, 1\n",
    "# model_suffix  = f\"qir{qir}_qnr{qnr}_nl{nl}_nr{nr}\"\n",
    "# model_config  = {\"num_ir_qubits\":qir, \"num_nr_qubits\":qnr, \"num_layers\":nl, \"num_reupload\":nr, \"lr\":1E-2, \"model_suffix\":model_suffix}\n",
    "# model         = QuantumRotQCGNN(**model_config)\n",
    "# train(model, model_config, data_module, data_config, graph=False, suffix=\"\")\n",
    "\n",
    "# QFCGNN\n",
    "data_module   = JetDataModule(sig_events, bkg_events, graph=False)\n",
    "qidx, qnn     = int(np.ceil(np.log2(data_config[\"num_pt_ptcs\"]))), 3\n",
    "gl, gr        = 1, 1\n",
    "model_suffix  = f\"qidx{qidx}_qnn{qnn}_gl{gl}_gr{gr}\"\n",
    "model_config  = {\"gnn_idx_qubits\":qidx, \"gnn_nn_qubits\":qnn, \"gnn_layers\":gl, \"gnn_reupload\":gr, \"lr\":1E-3, \"model_suffix\":model_suffix}\n",
    "model         = QuantumRotFCGNN(num_ir_qubits=qidx, num_nr_qubits=qnn, num_layers=gl, num_reupload=gr)\n",
    "train(model, model_config, data_module, data_config, graph=False, suffix=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "abdd0d95ca50f233d1202cce1ba28eab5ada50f7ec17823ef40ef9b79347f6f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
