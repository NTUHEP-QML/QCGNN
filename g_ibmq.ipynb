{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import g_main\n",
    "import module_model\n",
    "\n",
    "QCGNN = module_model.QCGNN_IX\n",
    "\n",
    "shots = 1024\n",
    "\n",
    "general_config = g_main.json_config.copy()\n",
    "general_config[\"batch_size\"] = 10\n",
    "general_config[\"rnd_seed\"] = 0\n",
    "general_config[\"max_num_ptcs\"] = 2\n",
    "general_config[\"num_bin_data\"] = 100 # {100} * 10 * 2 * (1 - 0.8) = 400 test data\n",
    "L.seed_everything(general_config[\"rnd_seed\"])\n",
    "\n",
    "# Quantum configurations.\n",
    "qidx = 1\n",
    "qnn, gl, gr  = 7, 1, 7\n",
    "\n",
    "# Output directory.\n",
    "ibmq_dir = os.path.join(general_config[\"predictions_dir\"], \"ibmq\")\n",
    "os.makedirs(ibmq_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IBMQQCGNN(QCGNN):\n",
    "    \"\"\"Return additional measurement outputs.\"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pt = x[..., 0]\n",
    "        num_ptcs = torch.sum((pt > 0).float(), axis=-1, keepdim=True)\n",
    "        x = torch.flatten(x, start_dim=-2, end_dim=-1)\n",
    "        x = self.net(x)\n",
    "        x = torch.unflatten(\n",
    "            x, dim=-1, sizes=(2**self.num_ir_qubits, self.num_nr_qubits))\n",
    "        \n",
    "        # Add this line.\n",
    "        meas = x.detach() # Measurement outputs.\n",
    "\n",
    "        x = x.mT\n",
    "        x = torch.sum(x, dim=-1)\n",
    "\n",
    "        if self.aggregation == \"SUM\":\n",
    "            x = x * num_ptcs\n",
    "        elif self.aggregation == \"MEAN\":\n",
    "            x = x / num_ptcs\n",
    "        \n",
    "        return x, meas\n",
    "\n",
    "class IBMQQuantumRotQCGNN(g_main.QuantumRotQCGNN):\n",
    "    def __init__(self, num_ir_qubits, num_nr_qubits, num_layers, num_reupload, quantum_config):\n",
    "        \"\"\"Change phi from QCGNN to IBMQQCGNN.\"\"\"\n",
    "        \n",
    "        super().__init__(\n",
    "            num_ir_qubits=num_ir_qubits,\n",
    "            num_nr_qubits=num_nr_qubits,\n",
    "            num_layers=num_layers,\n",
    "            num_reupload=num_reupload,\n",
    "            quantum_config=quantum_config,\n",
    "            aggregation=\"SUM\",\n",
    "        )\n",
    "        \n",
    "        # Substitute QCGNN with IBMQQCGNN.\n",
    "        self.phi = IBMQQCGNN(\n",
    "            num_ir_qubits=num_ir_qubits,\n",
    "            num_nr_qubits=num_nr_qubits,\n",
    "            num_layers=num_layers,\n",
    "            num_reupload=num_reupload,\n",
    "            ctrl_enc=self.ctrl_enc,\n",
    "            shots=shots,\n",
    "            **quantum_config\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Additional measurement outputs from IBMQQCGNN.\n",
    "        x, meas = self.phi(x)\n",
    "\n",
    "        # Original workflow.\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x.detach(), meas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_state_dict(model: nn.Module, ckpt_path: str):\n",
    "    \"\"\"Load model checkpoints parameters.\n",
    "    \n",
    "    Args:\n",
    "        model : nn.Module\n",
    "            Model to be load state dict.\n",
    "        ckpt_path : str\n",
    "            Path of checkpoints.\n",
    "    \"\"\"\n",
    "\n",
    "    # The keys of ckpt state_dict somehow different than loading keys.\n",
    "    old_state_dict = torch.load(ckpt_path)[\"state_dict\"]\n",
    "    new_state_dict = {}\n",
    "    for old_key in old_state_dict.keys():\n",
    "        # Containing prefix \"model.\", no need this prefix.\n",
    "        new_key = old_key[6:]\n",
    "        print(f\"state_dict key updated: {old_key} ---> {new_key}\")\n",
    "        new_state_dict[new_key] = old_state_dict[old_key]\n",
    "    model.load_state_dict(new_state_dict)\n",
    "\n",
    "def prediction(x, data_config, quantum_config):\n",
    "    # Choose pretrained model with number of particles = 16 -> qidx = log2(16) = 43\n",
    "    model_suffix = f\"qidx4_qnn{qnn}_gl{gl}_gr{gr}\"\n",
    "\n",
    "    # Get checkpoints file.\n",
    "    data_suffix = f\"{data_config['abbrev']}_ptc16_thres0.05-nb500_R0\"\n",
    "    ckpt_key = f\"QCGNN_IX_{model_suffix}_SUM-{data_suffix}\"\n",
    "    ckpt_path = g_main.get_ckpt_path(ckpt_key, general_config[\"rnd_seed\"])\n",
    "    \n",
    "    # Create model and load checkpoints.\n",
    "    model = IBMQQuantumRotQCGNN(qidx, qnn, gl, gr, quantum_config)\n",
    "    load_state_dict(model, ckpt_path)\n",
    "    model.eval()\n",
    "    \n",
    "    return model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate a new data configuration list.\n",
    "data_config_list = [\n",
    "    # 2-prong v.s. 1-prong.\n",
    "    g_main.generate_data_config(\n",
    "        sig=\"VzToZhToVevebb\", bkg=\"VzToQCD\", abbrev=\"BB-QCD\",\n",
    "        cut_pt=(800, 1000), subjet_radius=0, bin=10,\n",
    "        num_bin_data=general_config[\"num_bin_data\"],\n",
    "        max_num_ptcs=general_config[\"max_num_ptcs\"],\n",
    "        pad_num_ptcs=(2 ** qidx),\n",
    "        pt_threshold=general_config[\"pt_threshold\"],\n",
    "        num_ptcs_range=(2 ** qidx, 2 ** qidx),\n",
    "        print_log=True,\n",
    "    ),\n",
    "    \n",
    "    # 3-prong v.s. 1-prong.\n",
    "    g_main.generate_data_config(\n",
    "        sig=\"VzToTt\", bkg=\"VzToQCD\", abbrev=\"TT-QCD\",\n",
    "        cut_pt=(800, 1000), subjet_radius=0, bin=10,\n",
    "        num_bin_data=general_config[\"num_bin_data\"],\n",
    "        max_num_ptcs=general_config[\"max_num_ptcs\"],\n",
    "        pad_num_ptcs=(2 ** qidx),\n",
    "        pt_threshold=general_config[\"pt_threshold\"],\n",
    "        num_ptcs_range=(2 ** qidx, 2 ** qidx),\n",
    "        print_log=True,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an IBMQ backend\n",
    "ibmq_backend = input(\"Enter IBMQ backend = \") or \"ibmq_qasm_simulator\"\n",
    "\n",
    "# Prediction on IBMQ.\n",
    "for data_config in data_config_list:\n",
    "    \n",
    "    # Create the data module.\n",
    "    data_module = g_main.generate_datamodule(\n",
    "        data_config=data_config,\n",
    "        data_ratio=general_config[\"data_ratio\"],\n",
    "        batch_size=general_config[\"batch_size\"],\n",
    "        graph=False\n",
    "    )\n",
    "\n",
    "    # Buffers for saving data.\n",
    "    x, y_true, y_penl, meas_penl, y_ibmq, meas_ibmq = [torch.tensor([]) for _ in range(6)]\n",
    "    for _x, _y_true in tqdm(data_module.test_dataloader()):\n",
    "        # Prediction on PennyLane.\n",
    "        _y_penl, _meas_penl = prediction(\n",
    "            x=_x,\n",
    "            data_config=data_config,\n",
    "            quantum_config={\"qdevice\": \"default.qubit\", \"diff_method\": \"best\", \"qbackend\": \"\"}\n",
    "        )\n",
    "\n",
    "        # Prediction on IBMQ.\n",
    "        _y_ibmq, _meas_ibmq = prediction(\n",
    "            x=_x,\n",
    "            data_config=data_config,\n",
    "            quantum_config={\"qdevice\": \"qiskit.ibmq\", \"diff_method\": \"parameter-shift\", \"qbackend\": ibmq_backend}\n",
    "        )\n",
    "\n",
    "        # Dataset and true labels.\n",
    "        x, y_true = torch.cat((x, _x)), torch.cat((y_true, _y_true))\n",
    "        \n",
    "        # Save predictions to buffers.\n",
    "        y_penl, meas_penl = torch.cat((y_penl, _y_penl)), torch.cat((meas_penl, _meas_penl))\n",
    "        y_ibmq, meas_ibmq = torch.cat((y_ibmq, _y_ibmq)), torch.cat((meas_ibmq, _meas_ibmq))\n",
    "\n",
    "    # Output dictionary    \n",
    "    result = {\n",
    "        \"x\": x, \"y_true\": y_true,\n",
    "        \"y_penl\": y_penl, \"meas_penl\": meas_penl,\n",
    "        \"y_ibmq\": y_ibmq, \"meas_ibmq\": meas_ibmq\n",
    "    }\n",
    "\n",
    "    # Save result to npy files.\n",
    "    npy_file = (\n",
    "        # backend - shots - abbrev\n",
    "        f\"{ibmq_backend}-s{shots}-{data_config['abbrev']}_\"\n",
    "\n",
    "        # rnd_seed - max_num_ptcs\n",
    "        + f\"r{general_config['rnd_seed']}-p{general_config['max_num_ptcs']}_\"\n",
    "\n",
    "        # num_bin_data\n",
    "        + f\"n{general_config['num_bin_data']}_\"\n",
    "\n",
    "        # batch_size - qnn_gl_gr\n",
    "        + f\"b{general_config['batch_size']}-{qnn}_{gl}_{gr}\"\n",
    "\n",
    "        # npy suffix.\n",
    "        + \".npy\"\n",
    "    )\n",
    "    npy_path = os.path.join(ibmq_dir, npy_file)\n",
    "    np.save(npy_path, result, allow_pickle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
