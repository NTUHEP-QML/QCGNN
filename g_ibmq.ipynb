{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "\n",
    "import g_main\n",
    "import module_model\n",
    "\n",
    "shots = 1024\n",
    "\n",
    "config = g_main.config\n",
    "config[\"batch_size\"]   = 10 # 300\n",
    "config[\"rnd_seed\"]     = 2\n",
    "config[\"num_pt_ptcs\"]  = 4\n",
    "config[\"num_bin_data\"] = 25 # 25 * 10 * 2 * (1-0.8) = 100 test data\n",
    "L.seed_everything(config[\"rnd_seed\"])\n",
    "\n",
    "ibmq_dir = \"./ibmq\"\n",
    "os.makedirs(ibmq_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IBMQQCGNN(module_model.QCGNN):\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = torch.unflatten(x, dim=-1, sizes=(2**self.num_ir_qubits, self.num_nr_qubits))\n",
    "        x = x.mT\n",
    "        meas = x.detach()\n",
    "        x = torch.sum(x, dim=-1) * self.scale\n",
    "        return x, meas\n",
    "\n",
    "class QuantumRotQCGNN(nn.Module):\n",
    "    def __init__(self, num_ir_qubits, num_nr_qubits, num_layers, num_reupload, quantum_config):\n",
    "        super().__init__()\n",
    "        if \"qiskit\" in quantum_config[\"qdevice\"]:\n",
    "            ctrl_enc = lambda _input, control_values: g_main.qiskit_encoding(_input, control_values, num_ir_qubits, num_nr_qubits)\n",
    "        else:\n",
    "            ctrl_enc = lambda _input, control_values: g_main.pennylane_encoding(_input, control_values, num_ir_qubits, num_nr_qubits)\n",
    "        self.phi = IBMQQCGNN(num_ir_qubits, num_nr_qubits, num_layers, num_reupload, ctrl_enc=ctrl_enc, shots=shots, **quantum_config)\n",
    "        self.mlp = module_model.ClassicalMLP(in_channel=num_nr_qubits, out_channel=1, hidden_channel=0, num_layers=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, meas = self.phi(x)\n",
    "        x = self.mlp(x)\n",
    "        return x.detach(), meas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qidx = int(np.ceil(np.log2(config[\"num_pt_ptcs\"])))\n",
    "qnn, gl, gr  = 6, 1, 6\n",
    "data_configs = [\n",
    "    {\"sig\": \"VzToZhToVevebb\", \"bkg\": \"VzToQCD\", \"abbrev\":\"BB-QCD\", \"cut\": (800, 1000), \"bin\":10, \"subjet_radius\":0, \"num_bin_data\":config[\"num_bin_data\"], \"num_pt_ptcs\":config[\"num_pt_ptcs\"]},\n",
    "    {\"sig\": \"VzToTt\", \"bkg\": \"VzToQCD\", \"abbrev\":\"TT-QCD\", \"cut\": (800, 1000), \"bin\":10, \"subjet_radius\":0, \"num_bin_data\":config[\"num_bin_data\"], \"num_pt_ptcs\":config[\"num_pt_ptcs\"]},\n",
    "]\n",
    "\n",
    "def load_state_dict(model, ckpt_path):\n",
    "    old_state_dict = torch.load(ckpt_path)[\"state_dict\"]\n",
    "    new_state_dict = {}\n",
    "    for old_key in old_state_dict.keys():\n",
    "        new_key = old_key[6:]\n",
    "        print(f\"ModelLog: {old_key} ---> {new_key}\")\n",
    "        new_state_dict[new_key] = old_state_dict[old_key]\n",
    "    model.load_state_dict(new_state_dict)\n",
    "\n",
    "def prediction(x, quantum_config):\n",
    "    # load model\n",
    "    model_name   = QuantumRotQCGNN.__name__\n",
    "    model_suffix = f\"qidx3_qnn{qnn}_gl{gl}_gr{gr}\"\n",
    "    ckpt_key  = f\"{model_name}_{model_suffix} | {data_config['abbrev']}_cut{data_config['cut']}\"\n",
    "    ckpt_path = g_main.get_ckpt_path(ckpt_key)\n",
    "    model     = QuantumRotQCGNN(qidx, qnn, gl, gr, quantum_config)\n",
    "    load_state_dict(model, ckpt_path)\n",
    "    model.eval()\n",
    "    return model(x)\n",
    "\n",
    "ibmq_backend = input(\"Enter IBMQ backend = \")\n",
    "for data_config in data_configs:\n",
    "    data_module = g_main.generate_datamodule(data_config, graph=False)\n",
    "    x, y_true, y_penl, meas_penl, y_ibmq, meas_ibmq = [torch.tensor([]) for _ in range(6)]\n",
    "    for _x, _y_true in data_module.test_dataloader():\n",
    "        _y_penl, _meas_penl = prediction(_x, {\"qdevice\": \"default.qubit\", \"diff_method\": \"best\", \"qbackend\": \"\"})\n",
    "        _y_ibmq, _meas_ibmq = prediction(_x, {\"qdevice\": \"qiskit.ibmq\", \"diff_method\": \"parameter-shift\", \"qbackend\": ibmq_backend})\n",
    "        x, y_true = torch.cat((x, _x)), torch.cat((y_true, _y_true))\n",
    "        y_penl, meas_penl = torch.cat((y_penl, _y_penl)), torch.cat((meas_penl, _meas_penl))\n",
    "        y_ibmq, meas_ibmq = torch.cat((y_ibmq, _y_ibmq)), torch.cat((meas_ibmq, _meas_ibmq))\n",
    "    result = {\"x\":x, \"y_true\":y_true, \"y_penl\":y_penl, \"meas_penl\":meas_penl, \"y_ibmq\":y_ibmq, \"meas_ibmq\":meas_ibmq}\n",
    "    npy_file = os.path.join(ibmq_dir, f\"{ibmq_backend}-s{shots}-{data_config['abbrev']}_r{config['rnd_seed']}-p_{config['num_pt_ptcs']}_n{config['num_bin_data']}_b{config['batch_size']}-{qnn}_{gl}_{gr}.npy\")\n",
    "    np.save(npy_file, result, allow_pickle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
