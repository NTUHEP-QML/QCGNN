{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "import awkward as ak\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "\n",
    "from source.data.datamodule import JetTorchDataModule, JetGraphDataModule\n",
    "from source.data.opendata import JetNetEvents, TopQuarkEvents\n",
    "from source.models.mpgnn import ClassicalMPGNN\n",
    "from source.models.part import ParticleTransformer\n",
    "from source.models.pfn import ParticleFlowNetwork\n",
    "from source.models.pnet import ParticleNet\n",
    "from source.models.qcgnn import QuantumRotQCGNN\n",
    "from source.training.litmodel import TorchLightningModule, GraphLightningModel\n",
    "from source.training.loggers import csv_logger, wandb_logger\n",
    "from source.training.result import plot_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ipykernel' in sys.argv[0]:\n",
    "    # Jupyter notebook (.ipynb)\n",
    "    # dataset = 'TopQCD'\n",
    "    dataset = 'JetNet'\n",
    "    random_seed = 42\n",
    "    parser_args = None\n",
    "\n",
    "else:\n",
    "    # Python script (.py)\n",
    "    parser = argparse.ArgumentParser(description='Process some inputs.')\n",
    "    parser.add_argument('--dataset', type=str, default=None, help='Dataset (default: None)')\n",
    "    parser.add_argument('--random_seed', type=int, default=42, help='Random seed (default: 42)')\n",
    "    parser.add_argument('--num_train', type=int, default=None, help='Number of training data (default: None)')\n",
    "    parser.add_argument('--suffix', type=str, default=None, help='Suffix (default: None)')\n",
    "    parser_args = parser.parse_args()\n",
    "\n",
    "    dataset = parser_args.dataset\n",
    "    random_seed = parser_args.random_seed\n",
    "\n",
    "with open(f\"configs/config.yaml\", 'r') as file:\n",
    "    \n",
    "    # Configuration of training.\n",
    "    config = yaml.safe_load(file)\n",
    "    config['date'] = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n",
    "    config['dataset'] = dataset\n",
    "\n",
    "    # Whether change default number of data\n",
    "    if parser_args is not None:\n",
    "        if parser_args.num_train is not None:\n",
    "            config['Data']['num_train'] = parser_args.num_train\n",
    "            config['Data']['num_val'] = int(0.1 * parser_args.num_train)\n",
    "            config['Data']['num_test'] = int(0.1 * parser_args.num_train)\n",
    "        if parser_args.suffix is not None:\n",
    "            config['Settings']['suffix'] = parser_args.suffix\n",
    "\n",
    "    # Determine the dimension of the score.\n",
    "    if config[dataset]['num_classes'] <= 2:\n",
    "        # Will use `BCELossWithLogits`\n",
    "        score_dim = 1\n",
    "    else:\n",
    "        # Will use `CrossEntropyLoss`\n",
    "        score_dim = config[dataset]['num_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_module(graph: bool, pi_scale: bool = False) -> L.LightningDataModule:\n",
    "    \"\"\"Randomly create a data module.\"\"\"\n",
    "    \n",
    "    # Read jet data (reading is not affected by random seed).\n",
    "    num_train = config['Data']['num_train']\n",
    "    num_valid = config['Data']['num_valid']\n",
    "    num_test = config['Data']['num_test']\n",
    "    num_events = num_train + num_valid + num_test\n",
    "\n",
    "    # Dataset settings.\n",
    "    dataset_config = {}\n",
    "    dataset_config.update(config['Data'])\n",
    "    dataset_config.update(config[dataset])\n",
    "\n",
    "    # JetNet dataset (multi-class classification).\n",
    "    if dataset == 'JetNet':\n",
    "        channels = ['q', 'g', 't', 'w', 'z']\n",
    "        events = [JetNetEvents(channel=channel, **dataset_config) for channel in channels]\n",
    "        events = [_events.generate_events(num_events) for _events in events]\n",
    "\n",
    "    # Top quark tagging dataset (background light quark QCD).\n",
    "    elif dataset == 'TopQCD':\n",
    "        events = []\n",
    "        for y, channel in enumerate(['Top', 'QCD']):\n",
    "            events_train = TopQuarkEvents(mode='train', is_signal_new=y, **dataset_config).generate_events(num_train)\n",
    "            events_valid = TopQuarkEvents(mode='valid', is_signal_new=y, **dataset_config).generate_events(num_valid)\n",
    "            events_test  = TopQuarkEvents(mode='test',  is_signal_new=y, **dataset_config).generate_events(num_test)\n",
    "            events.append(ak.concatenate([events_train, events_valid, events_test], axis=0))\n",
    "    \n",
    "    # Turn into data module (for lightning training module).\n",
    "    if graph:\n",
    "        data_module = JetGraphDataModule(events, pi_scale=pi_scale, **dataset_config)\n",
    "    else:\n",
    "        data_module = JetTorchDataModule(events, **dataset_config)\n",
    "\n",
    "    return data_module\n",
    "\n",
    "\n",
    "def create_training_info(model: nn.Module, model_description: str, model_hparams: dict, lr: float) -> dict:\n",
    "    \"\"\"Create a training information dictionary that will be recorded.\"\"\"\n",
    "\n",
    "    # Data information.\n",
    "    data_description = f\"{dataset}_P{config['Data']['max_num_ptcs']}_N{config['Data']['num_train']}\"\n",
    "\n",
    "    # The description used for grouping different result of random seeds.\n",
    "    group_rnd = '-'.join([model.__class__.__name__, model_description, data_description])\n",
    "    \n",
    "    # Name for this particular training (including random seed).\n",
    "    name = group_rnd\n",
    "    if config['Settings']['suffix'] != '':\n",
    "        name += '-' + config['Settings']['suffix']\n",
    "    name += '-' + str(random_seed)\n",
    "\n",
    "    # Hyperparameters and configurations that will be recorded.\n",
    "    training_info = config.copy()\n",
    "    training_info.update(model_hparams)\n",
    "    training_info.update({\n",
    "        'lr': lr,\n",
    "        'name': name,\n",
    "        'date': config['date'],\n",
    "        'model': model.__class__.__name__,\n",
    "        'group_rnd': group_rnd,\n",
    "        'random_seed': random_seed,\n",
    "        'data_description': data_description,\n",
    "        'model_description': model_description,\n",
    "    })\n",
    "\n",
    "    return training_info\n",
    "\n",
    "    \n",
    "def create_lightning_model(model: nn.Module, graph: bool, lr: float) -> L.LightningModule:\n",
    "    \"\"\"Create a lightning model for trainer.\"\"\"\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = torch.optim.RAdam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Create lightning model depends on graph or not.\n",
    "    print_log = config['Settings']['print_log']\n",
    "\n",
    "    # Graph is for PFN.\n",
    "    if graph:\n",
    "        return GraphLightningModel(model, optimizer=optimizer, score_dim=score_dim, print_log=print_log)\n",
    "    else:\n",
    "        return TorchLightningModule(model, optimizer=optimizer, score_dim=score_dim, print_log=print_log)\n",
    "\n",
    "\n",
    "def create_trainer(model: nn.Module, training_info: dict, accelerator: str) -> L.Trainer:\n",
    "    \"\"\"Create lightning trainer for training.\"\"\"\n",
    "\n",
    "    # Create logger for monitoring the training.\n",
    "    if config['Settings']['use_wandb']:\n",
    "        wandb.login()\n",
    "        logger = wandb_logger(training_info)\n",
    "        logger.watch(model)\n",
    "        loggers = [logger, csv_logger(training_info)]\n",
    "    else:\n",
    "        loggers = csv_logger(training_info)\n",
    "    \n",
    "    # Return the lightning trainer.\n",
    "    return L.Trainer(\n",
    "        logger=loggers,\n",
    "        accelerator=accelerator,\n",
    "        max_epochs=config['Train']['max_epochs'],\n",
    "        log_every_n_steps=config['Train']['log_every_n_steps'],\n",
    "        num_sanity_val_steps=config['Train']['num_sanity_val_steps'],\n",
    "        callbacks=[ModelCheckpoint(\n",
    "            monitor=config['Train']['ckpt_monitor'],\n",
    "            mode=config['Train']['ckpt_mode'],\n",
    "            save_top_k=config['Train']['ckpt_top_k'],\n",
    "            save_last=True,\n",
    "            filename='{epoch}-{valid_auc:.3f}-{valid_accuracy:.3f}',\n",
    "        )],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: nn.Module, model_description: str, model_hparams: dict,\n",
    "        accelerator: str, lr: float, graph: bool, pi_scale: bool = False\n",
    "    ):\n",
    "    \n",
    "    # Fix all random stuff.\n",
    "    L.seed_everything(random_seed)\n",
    "\n",
    "    # Traditional training procedure.\n",
    "    data_module = create_data_module(graph=graph, pi_scale=pi_scale)\n",
    "    lightning_model = create_lightning_model(model=model, graph=graph, lr=lr)\n",
    "    training_info = create_training_info(model, model_description, model_hparams, lr=lr)\n",
    "    trainer = create_trainer(model, training_info, accelerator)\n",
    "    \n",
    "    # Training and validation.\n",
    "    if eval(config['Settings']['mode'][0]):\n",
    "        if eval(config['Settings']['mode'][1]):\n",
    "            trainer.fit(lightning_model, datamodule=data_module)\n",
    "        else:\n",
    "            trainer.fit(lightning_model, train_dataloaders=data_module.train_dataloader())\n",
    "\n",
    "    # Testing.\n",
    "    if eval(config['Settings']['mode'][2]):\n",
    "        trainer.test(lightning_model, datamodule=data_module, ckpt_path='best')\n",
    "\n",
    "    # Finish wandb if used.\n",
    "    if config['Settings']['use_wandb']:\n",
    "        wandb.finish()\n",
    "\n",
    "    return training_info['name']\n",
    "\n",
    "def train_quantum(model_class: nn.Module, model_hparams: dict, pi_scale: bool, lr: float):\n",
    "\n",
    "    model = model_class(score_dim=score_dim, **model_hparams)\n",
    "\n",
    "    num_ir_qubits = model_hparams['num_ir_qubits']\n",
    "    num_nr_qubits = model_hparams['num_nr_qubits']\n",
    "    num_layers = model_hparams['num_layers']\n",
    "    num_reupload = model_hparams['num_reupload']\n",
    "    dropout = model_hparams['dropout']\n",
    "    model_description = f\"nI{num_ir_qubits}_nQ{num_nr_qubits}_L{num_layers}_R{num_reupload}_D{dropout:.2f}\"\n",
    "\n",
    "    accelerator = 'cpu'\n",
    "    name = train(model, model_description, model_hparams, accelerator, lr=lr, graph=False, pi_scale=pi_scale)\n",
    "\n",
    "    return name\n",
    "\n",
    "def train_mpgnn(model_hparams: dict, lr: float):\n",
    "    \n",
    "    model = ClassicalMPGNN(score_dim=score_dim, **model_hparams)\n",
    "\n",
    "    phi_out = model_hparams['phi_out']\n",
    "    phi_hidden = model_hparams['phi_hidden']\n",
    "    phi_layers = model_hparams['phi_layers']\n",
    "    dropout = model_hparams['dropout']\n",
    "    model_description = f\"O{phi_out}_H{phi_hidden}_L{phi_layers}_D{dropout:.2f}\"\n",
    "\n",
    "    accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "    name = train(model, model_description, model_hparams, accelerator, lr=lr, graph=True)\n",
    "    \n",
    "    return name\n",
    "\n",
    "def train_benchmark(model_class: nn.Module, lr: float):\n",
    "    with open('configs/benchmark.yaml', 'r') as file:\n",
    "        hparams = yaml.safe_load(file)[model_class.__name__]\n",
    "        model_description = ''\n",
    "\n",
    "    model = model_class(score_dim=score_dim, parameters=hparams)\n",
    "    \n",
    "    accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    if model_class == ParticleFlowNetwork:\n",
    "        name = train(model, model_description, hparams, accelerator, lr=lr, graph=True)\n",
    "    else:\n",
    "        name = train(model, model_description, hparams, accelerator, lr=lr, graph=False)\n",
    "    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # QCGNN\n",
    "# for Q in [3, 6]:\n",
    "#     print(f\"\\n* Train QCGNN n_Q = {Q}.\\n\")\n",
    "#     qcgnn_hparams = {'num_ir_qubits': 4, 'num_nr_qubits': Q, 'num_layers': Q // 3, 'num_reupload': 2, 'dropout': 0.0, 'vqc_ansatz': qml.StronglyEntanglingLayers}\n",
    "#     name = train_quantum(model_class=QuantumRotQCGNN, model_hparams=qcgnn_hparams, pi_scale=True, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MPGNN (3, 6)\n",
    "# for phi_dim in [3, 6]:\n",
    "#     pfn_hparams = {'phi_in': 3 + 3, 'phi_out': phi_dim, 'phi_layers': 2, 'phi_hidden': phi_dim, 'mlp_hidden': 16, 'dropout': 0.0}\n",
    "#     name = train_mpgnn(model_hparams=pfn_hparams, lr=1e-3)\n",
    "\n",
    "# # MPGNN (64)\n",
    "# pfn_hparams = {'phi_in': 3 + 3, 'phi_out': 64, 'phi_layers': 2, 'phi_hidden': 64, 'mlp_hidden': 64, 'dropout': 0.0}\n",
    "# name = train_mpgnn(model_hparams=pfn_hparams, lr=1e-3)\n",
    "\n",
    "# # Particle Flow Network.\n",
    "# print(f\"\\n* Train Particle Flow Network.\\n\")\n",
    "# name = train_benchmark(model_class=ParticleFlowNetwork, lr=1e-3)\n",
    "\n",
    "# # Particle Transformer.\n",
    "# print(f\"\\n* Train Particle Transformer.\\n\")\n",
    "# name = train_benchmark(model_class=ParticleTransformer, lr=1e-3)\n",
    "\n",
    "# # Particle Net.\n",
    "# print(f\"\\n* Train Particle Net.\\n\")\n",
    "# name = train_benchmark(model_class=ParticleNet, lr=1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
