{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "import awkward as ak\n",
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "\n",
    "from source.data.datamodule import JetTorchDataModule, JetGraphDataModule\n",
    "from source.data.opendata import JetNetEvents, TopQuarkEvents\n",
    "from source.models.qcgnn import QuantumRotQCGNN, HybridQCGNN\n",
    "from source.models.mpgnn import ClassicalMPGNN\n",
    "from source.models.part import ParticleTransformer\n",
    "from source.models.pnet import ParticleNet\n",
    "from source.training.litmodel import TorchLightningModule, GraphLightningModel\n",
    "from source.training.loggers import csv_logger, wandb_logger\n",
    "from source.training.result import plot_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ipykernel' in sys.argv[0]:\n",
    "    # Jupyter notebook (.ipynb)\n",
    "    # dataset = 'TopQCD'\n",
    "    dataset = 'JetNet'\n",
    "    random_seed = 42\n",
    "\n",
    "else:\n",
    "    # Python script (.py)\n",
    "    parser = argparse.ArgumentParser(description='Process some inputs.')\n",
    "    parser.add_argument('--dataset', type=str, default=None, help='Dataset (default: None)')\n",
    "    parser.add_argument('--random_seed', type=int, default=42, help='Random seed (default: 42)')\n",
    "    parser_args = parser.parse_args()\n",
    "\n",
    "    dataset = parser_args.dataset\n",
    "    random_seed = parser_args.random_seed\n",
    "\n",
    "with open(f\"configs/config.yaml\", 'r') as file:\n",
    "    \n",
    "    # Configuration of training.\n",
    "    config = yaml.safe_load(file)\n",
    "    config['date'] = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n",
    "    config['dataset'] = dataset\n",
    "\n",
    "    # Determine the dimension of the score.\n",
    "    if config[dataset]['num_classes'] <= 2:\n",
    "        # Will use `BCELossWithLogits`\n",
    "        score_dim = 1\n",
    "    else:\n",
    "        # Will use `CrossEntropyLoss`\n",
    "        score_dim = config[dataset]['num_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_module(graph: bool, pi_scale: bool = False) -> L.LightningDataModule:\n",
    "    \"\"\"Randomly create a data module.\"\"\"\n",
    "    \n",
    "    # Read jet data (reading is not affected by random seed).\n",
    "    num_train = config['Data']['num_train']\n",
    "    num_valid = config['Data']['num_valid']\n",
    "    num_test = config['Data']['num_test']\n",
    "    num_events = num_train + num_valid + num_test\n",
    "\n",
    "    # Dataset settings.\n",
    "    dataset_config = {}\n",
    "    dataset_config.update(config['Data'])\n",
    "    dataset_config.update(config[dataset])\n",
    "\n",
    "    # JetNet dataset (multi-class classification).\n",
    "    if dataset == 'JetNet':\n",
    "        channels = ['q', 'g', 't', 'w', 'z']\n",
    "        events = [JetNetEvents(channel=channel, **dataset_config) for channel in channels]\n",
    "        events = [_events.generate_events(num_events) for _events in events]\n",
    "\n",
    "    # Top quark tagging dataset (background light quark QCD).\n",
    "    elif dataset == 'TopQCD':\n",
    "        events = []\n",
    "        for y, channel in enumerate(['Top', 'QCD']):\n",
    "            events_train = TopQuarkEvents(mode='train', is_signal_new=y, **dataset_config).generate_events(num_train)\n",
    "            events_valid = TopQuarkEvents(mode='valid', is_signal_new=y, **dataset_config).generate_events(num_valid)\n",
    "            events_test  = TopQuarkEvents(mode='test',  is_signal_new=y, **dataset_config).generate_events(num_test)\n",
    "            events.append(ak.concatenate([events_train, events_valid, events_test], axis=0))\n",
    "    \n",
    "    # Turn into data module (for lightning training module).\n",
    "    if graph:\n",
    "        data_module = JetGraphDataModule(events, pi_scale=pi_scale, **dataset_config)\n",
    "    else:\n",
    "        data_module = JetTorchDataModule(events, **dataset_config)\n",
    "\n",
    "    return data_module\n",
    "\n",
    "\n",
    "def create_training_info(model: nn.Module, model_description: str, model_hparams: dict) -> dict:\n",
    "    \"\"\"Create a training information dictionary that will be recorded.\"\"\"\n",
    "\n",
    "    # Data information.\n",
    "    data_description = f\"{dataset}_P{config['Data']['max_num_ptcs']}_N{config['Data']['num_train']}\"\n",
    "\n",
    "    # The description used for grouping different result of random seeds.\n",
    "    group_rnd = '-'.join([model.__class__.__name__, model_description, data_description])\n",
    "    \n",
    "    # Name for this particular training (including random seed).\n",
    "    name = group_rnd\n",
    "    if config['Settings']['suffix'] != '':\n",
    "        name += '-' + config['Settings']['suffix']\n",
    "    name += '-' + str(random_seed)\n",
    "\n",
    "    # Hyperparameters and configurations that will be recorded.\n",
    "    training_info = config.copy()\n",
    "    training_info.update(model_hparams)\n",
    "    training_info.update({\n",
    "        'name': name,\n",
    "        'date': config['date'],\n",
    "        'model': model.__class__.__name__,\n",
    "        'group_rnd': group_rnd,\n",
    "        'random_seed': random_seed,\n",
    "        'data_description': data_description,\n",
    "        'model_description': model_description,\n",
    "    })\n",
    "\n",
    "    return training_info\n",
    "\n",
    "    \n",
    "def create_lightning_model(model: nn.Module, graph: bool) -> L.LightningModule:\n",
    "    \"\"\"Create a lightning model for trainer.\"\"\"\n",
    "\n",
    "    # Optimizer.\n",
    "    lr = eval(config['Train']['lr'])\n",
    "    optimizer = torch.optim.RAdam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Create lightning model depends on graph or not.\n",
    "    print_log = config['Settings']['print_log']\n",
    "\n",
    "    # Graph is for MPGNN.\n",
    "    if graph:\n",
    "        return GraphLightningModel(model, optimizer=optimizer, score_dim=score_dim, print_log=print_log)\n",
    "    else:\n",
    "        return TorchLightningModule(model, optimizer=optimizer, score_dim=score_dim, print_log=print_log)\n",
    "    \n",
    "\n",
    "def create_trainer(model: nn.Module, training_info: dict, accelerator: str) -> L.Trainer:\n",
    "    \"\"\"Create lightning trainer for training.\"\"\"\n",
    "\n",
    "    # Create logger for monitoring the training.\n",
    "    if config['Settings']['use_wandb']:\n",
    "        wandb.login()\n",
    "        logger = wandb_logger(training_info)\n",
    "        logger.watch(model)\n",
    "        loggers = [logger, csv_logger(training_info)]\n",
    "    else:\n",
    "        loggers = csv_logger(training_info)\n",
    "    \n",
    "    # Return the lightning trainer.\n",
    "    return L.Trainer(\n",
    "        logger=loggers,\n",
    "        accelerator=accelerator,\n",
    "        max_epochs=config['Train']['max_epochs'],\n",
    "        log_every_n_steps=config['Train']['log_every_n_steps'],\n",
    "        num_sanity_val_steps=config['Train']['num_sanity_val_steps'],\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: nn.Module, model_description: str, model_hparams: dict,\n",
    "        accelerator: str, graph: bool, pi_scale: bool = False,\n",
    "    ):\n",
    "    \n",
    "    # Fix all random stuff.\n",
    "    L.seed_everything(random_seed)\n",
    "\n",
    "    # Traditional training procedure.\n",
    "    data_module = create_data_module(graph=graph, pi_scale=pi_scale)\n",
    "    lightning_model = create_lightning_model(model=model, graph=graph)\n",
    "    training_info = create_training_info(model, model_description, model_hparams)\n",
    "    trainer = create_trainer(model, training_info, accelerator)\n",
    "    \n",
    "    # Training and validation.\n",
    "    if eval(config['Settings']['mode'][0]):\n",
    "        if eval(config['Settings']['mode'][1]):\n",
    "            trainer.fit(lightning_model, datamodule=data_module)\n",
    "        else:\n",
    "            trainer.fit(lightning_model, train_dataloaders=data_module.train_dataloader())\n",
    "\n",
    "    # Testing.\n",
    "    if eval(config['Settings']['mode'][2]):\n",
    "        trainer.test(lightning_model, datamodule=data_module)\n",
    "\n",
    "    # Finish wandb if used.\n",
    "    if config['Settings']['use_wandb']:\n",
    "        wandb.finish()\n",
    "\n",
    "    return training_info['name']\n",
    "\n",
    "def train_quantum(model_class: nn.Module, model_hparams: dict, pi_scale: bool):\n",
    "\n",
    "    model = model_class(score_dim=score_dim, **model_hparams)\n",
    "\n",
    "    num_ir_qubits = model_hparams['num_ir_qubits']\n",
    "    num_nr_qubits = model_hparams['num_nr_qubits']\n",
    "    num_layers = model_hparams['num_layers']\n",
    "    num_reupload = model_hparams['num_reupload']\n",
    "    model_description = f\"nI{num_ir_qubits}_nQ{num_nr_qubits}_l{num_layers}_r{num_reupload}\"\n",
    "\n",
    "    accelerator = 'cpu'\n",
    "    name = train(model, model_description, model_hparams, accelerator, graph=False, pi_scale=pi_scale)\n",
    "\n",
    "    return name\n",
    "\n",
    "def train_mpgnn(model_hparams: dict):\n",
    "    \n",
    "    model = ClassicalMPGNN(score_dim=score_dim, **model_hparams)\n",
    "\n",
    "    gnn_out = model_hparams['gnn_out']\n",
    "    gnn_hidden = model_hparams['gnn_hidden']\n",
    "    gnn_layers = model_hparams['gnn_layers']\n",
    "    model_description = f\"go{gnn_out}_gh{gnn_hidden}_gl{gnn_layers}\"\n",
    "\n",
    "    accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "    name = train(model, model_description, model_hparams, accelerator, graph=True)\n",
    "    \n",
    "    return name\n",
    "\n",
    "def train_benchmark(model_class: nn.Module, lite: bool = False):\n",
    "    if lite:\n",
    "        with open('configs/benchmark_lite.yaml', 'r') as file:\n",
    "            hparams = yaml.safe_load(file)[model_class.__name__]\n",
    "            model_description = 'lite'\n",
    "    else:\n",
    "        with open('configs/benchmark.yaml', 'r') as file:\n",
    "            hparams = yaml.safe_load(file)[model_class.__name__]\n",
    "            model_description = ''\n",
    "\n",
    "    model = model_class(score_dim=score_dim, parameters=hparams)\n",
    "    \n",
    "    accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "    name = train(model, model_description, hparams, accelerator, graph=False)\n",
    "    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # QCGNN\n",
    "# for num_nr_qubits in [3, 5, 7]:\n",
    "#     print(f\"\\n* Train QCGNN {num_nr_qubits}.\\n\")\n",
    "#     qcgnn_hparams = {'num_ir_qubits': 4, 'num_nr_qubits': num_nr_qubits, 'num_layers': 1, 'num_reupload': num_nr_qubits}\n",
    "#     name = train_quantum(model_class=QuantumRotQCGNN, model_hparams=qcgnn_hparams, pi_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hybrid\n",
    "# for num_nr_qubits in [3, 5, 7]:\n",
    "#     print(f\"\\n* Train Hybrid {num_nr_qubits}.\\n\")\n",
    "#     qcgnn_hparams = {'num_ir_qubits': 4, 'num_nr_qubits': num_nr_qubits, 'num_layers': 1, 'num_reupload': num_nr_qubits}\n",
    "#     name = train_quantum(model_class=HybridQCGNN, model_hparams=qcgnn_hparams, pi_scale=False)dddddsdasdasdasdasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MPGNN\n",
    "\n",
    "for gnn_dim in [3, 5, 7, 64]:\n",
    "    print(f\"\\n* Train MPGNN {gnn_dim}.\\n\")\n",
    "    mpgnn_hparams = {'gnn_in': 6, 'gnn_out': gnn_dim, 'gnn_layers': 2, 'gnn_hidden': gnn_dim}\n",
    "    name = train_mpgnn(model_hparams=mpgnn_hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Particle Transformer (without interaction).\n",
    "\n",
    "print(f\"\\n* Train Particle Transformer.\\n\")\n",
    "name = train_benchmark(model_class=ParticleTransformer, lite=False)\n",
    "name = train_benchmark(model_class=ParticleTransformer, lite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Particle Net.\n",
    "\n",
    "print(f\"\\n* Train Particle Net.\\n\")\n",
    "name = train_benchmark(model_class=ParticleNet, lite=False)\n",
    "name = train_benchmark(model_class=ParticleNet, lite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
