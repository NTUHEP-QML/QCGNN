{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "\n",
    "from source.data.preprocessing import FatJetEvents\n",
    "from source.data.datamodule import JetTorchDataModule, JetGraphDataModule\n",
    "from source.models.qcgnn import QuantumRotQCGNN, HybridQCGNN\n",
    "from source.models.mpgnn import ClassicalMPGNN\n",
    "from source.models.part import ParticleTransformer\n",
    "from source.models.pnet import ParticleNet\n",
    "from source.training.litmodel import TorchLightningModule, GraphLightningModel\n",
    "from source.training.loggers import csv_logger, wandb_logger\n",
    "from source.training.result import plot_metrics\n",
    "\n",
    "with open('configs/config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    config['date'] = time.strftime('%Y%m%d_%H%M%S', time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_module(graph: bool) -> L.LightningDataModule:\n",
    "    \"\"\"Randomly create a data module.\"\"\"\n",
    "    \n",
    "    # Read jet data (reading is not affected by random seed).\n",
    "    channels = ['VzToQCD', 'VzToZH', 'VzToTT']\n",
    "    num_train = config['Data']['num_train']\n",
    "    num_valid = config['Data']['num_valid']\n",
    "    num_test = config['Data']['num_test']\n",
    "    num_bins = config['Data']['num_bins']\n",
    "    num_data_per_bin = math.ceil((num_train + num_valid + num_test) / num_bins)\n",
    "    events = [FatJetEvents(channel=channel, num_data_per_bin=num_data_per_bin, **config['Data']) for channel in channels]\n",
    "    events = [_events.generate_uniform_pt_events() for _events in events]\n",
    "\n",
    "    # Turn into data module (for lightning training module).\n",
    "    if graph:\n",
    "        data_module = JetGraphDataModule(events, **config['Data'])\n",
    "    else:\n",
    "        data_module = JetTorchDataModule(events, **config['Data'])\n",
    "\n",
    "    return data_module\n",
    "\n",
    "\n",
    "def create_training_info(model: nn.Module, model_description: str, model_hparams: dict, random_seed: int) -> dict:\n",
    "    \"\"\"Create a training information dictionary that will be recorded.\"\"\"\n",
    "\n",
    "    # Data information.\n",
    "    data_description = f\"P{config['Data']['max_num_ptcs']}_N{config['Data']['num_train']}\"\n",
    "\n",
    "    # The description used for grouping different result of random seeds.\n",
    "    group_rnd = '-'.join([model.__class__.__name__, model_description, data_description])\n",
    "    \n",
    "    # Name for this particular training (including random seed).\n",
    "    name = group_rnd\n",
    "    if config['Settings']['suffix'] != '':\n",
    "        name += '-' + config['Settings']['suffix']\n",
    "    name += '-' + str(random_seed)\n",
    "\n",
    "    # Hyperparameters and configurations that will be recorded.\n",
    "    training_info = config.copy()\n",
    "    training_info.update(model_hparams)\n",
    "    training_info.update({\n",
    "        'model_description': model_description,\n",
    "        'data_description': data_description,\n",
    "        'group_rnd': group_rnd,\n",
    "        'name': name,\n",
    "        'date': config['date'],\n",
    "        'model': model.__class__.__name__,\n",
    "        'random_seed': random_seed,\n",
    "    })\n",
    "\n",
    "    return training_info\n",
    "\n",
    "    \n",
    "def create_lightning_model(model: nn.Module, graph: bool) -> L.LightningModule:\n",
    "    \"\"\"Create a lightning model for trainer.\"\"\"\n",
    "\n",
    "    # Optimizer.\n",
    "    lr = eval(config['Train']['lr'])\n",
    "    optimizer = torch.optim.RAdam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Create lightning model depends on graph or not.\n",
    "    score_dim = config['Model']['score_dim']\n",
    "    print_log = config['Settings']['print_log']\n",
    "    if graph:\n",
    "        return GraphLightningModel(model, optimizer=optimizer, score_dim=score_dim, print_log=print_log)\n",
    "    else:\n",
    "        return TorchLightningModule(model, optimizer=optimizer, score_dim=score_dim, print_log=print_log)\n",
    "    \n",
    "\n",
    "def create_trainer(model: nn.Module, training_info: dict, accelerator: str) -> L.Trainer:\n",
    "    \"\"\"Create lightning trainer for training.\"\"\"\n",
    "\n",
    "    # Create logger for monitoring the training.\n",
    "    if config['Settings']['use_wandb']:\n",
    "        wandb.login()\n",
    "        logger = wandb_logger(training_info)\n",
    "        logger.watch(model)\n",
    "        loggers = [logger, csv_logger(training_info)]\n",
    "    else:\n",
    "        loggers = csv_logger(training_info)\n",
    "    \n",
    "    # Return the lightning trainer.\n",
    "    return L.Trainer(\n",
    "        logger=loggers,\n",
    "        accelerator=accelerator,\n",
    "        max_epochs=config['Train']['max_epochs'],\n",
    "        log_every_n_steps=config['Train']['log_every_n_steps'],\n",
    "        num_sanity_val_steps=config['Train']['num_sanity_val_steps'],\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: nn.Module, model_description: str, model_hparams: dict,\n",
    "        random_seed: int, accelerator: str, graph: bool,\n",
    "    ):\n",
    "    \n",
    "    # Fix all random stuff.\n",
    "    L.seed_everything(random_seed)\n",
    "\n",
    "    # Traditional training procedure.\n",
    "    data_module = create_data_module(graph=graph)\n",
    "    lightning_model = create_lightning_model(model=model, graph=graph)\n",
    "    training_info = create_training_info(model, model_description, model_hparams, random_seed)\n",
    "    trainer = create_trainer(model, training_info, accelerator)\n",
    "    \n",
    "    if config['Settings']['mode'] == '100':\n",
    "        trainer.fit(lightning_model, train_dataloaders=data_module.train_dataloader())\n",
    "    elif config['Settings']['mode'] == '110':\n",
    "        trainer.fit(lightning_model, datamodule=data_module)\n",
    "    elif config['Settings']['mode'] == '111':\n",
    "        trainer.fit(lightning_model, datamodule=data_module)\n",
    "        print('\\nTesting the model.\\n')\n",
    "        trainer.test(lightning_model, datamodule=data_module)\n",
    "\n",
    "    # Finish wandb if used.\n",
    "    if config['Settings']['use_wandb']:\n",
    "        wandb.finish()\n",
    "\n",
    "    return training_info['name']\n",
    "\n",
    "def train_quantum(random_seed: int, model_class: nn.Module, model_hparams: dict):\n",
    "\n",
    "    model_hparams.update(config['Model'])\n",
    "    model = model_class(**model_hparams)\n",
    "\n",
    "    num_ir_qubits = model_hparams['num_ir_qubits']\n",
    "    num_nr_qubits = model_hparams['num_nr_qubits']\n",
    "    num_layers = model_hparams['num_layers']\n",
    "    num_reupload = model_hparams['num_reupload']\n",
    "    model_description = f\"nI{num_ir_qubits}_nQ{num_nr_qubits}_l{num_layers}_r{num_reupload}\"\n",
    "\n",
    "    accelerator = 'cpu'\n",
    "    name = train(model, model_description, model_hparams, random_seed, accelerator, graph=False)\n",
    "\n",
    "    return name\n",
    "\n",
    "def train_mpgnn(random_seed: int, model_hparams: dict):\n",
    "    \n",
    "    model_hparams.update(config['Model'])\n",
    "    model = ClassicalMPGNN(**model_hparams)\n",
    "\n",
    "    gnn_out = model_hparams['gnn_out']\n",
    "    gnn_hidden = model_hparams['gnn_hidden']\n",
    "    gnn_layers = model_hparams['gnn_layers']\n",
    "    model_description = f\"go{gnn_out}_gh{gnn_hidden}_gl{gnn_layers}\"\n",
    "\n",
    "    accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "    name = train(model, model_description, model_hparams, random_seed, accelerator, graph=True)\n",
    "    \n",
    "    return name\n",
    "\n",
    "def train_benchmark(random_seed: int, model_class: nn.Module):\n",
    "    with open('configs/benchmark.yaml', 'r') as file:\n",
    "        hparams = yaml.safe_load(file)[model_class.__name__]\n",
    "\n",
    "    model = model_class(score_dim=config['Model']['score_dim'], parameters=hparams)\n",
    "    \n",
    "    accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "    name = train(model, model_class.__name__, hparams, random_seed, accelerator, graph=False)\n",
    "    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_list = []\n",
    "\n",
    "# for random_seed in range(3):\n",
    "#     print('=' * 10 + f\"Random Seed {random_seed}\" + '-' * 10)\n",
    "\n",
    "#     # MPGNN\n",
    "#     for gnn_dim in [3, 5, 7]:\n",
    "#         print(f\" * Train MPGNN {gnn_dim}.\")\n",
    "#         mpgnn_hparams = {'gnn_in': 6, 'gnn_out': gnn_dim, 'gnn_layers': 2, 'gnn_hidden': gnn_dim}\n",
    "#         name = train_mpgnn(random_seed=random_seed, model_hparams=mpgnn_hparams)\n",
    "#         name_list.append(name)\n",
    "\n",
    "#     # Super MPGNN\n",
    "#     print(f\" * Train MPGNN {256}.\")\n",
    "#     mpgnn_hparams = {'gnn_in': 6, 'gnn_out': 1024, 'gnn_layers': 2, 'gnn_hidden': 1024}\n",
    "#     name = train_mpgnn(random_seed=random_seed, model_hparams=mpgnn_hparams)\n",
    "#     name_list.append(name)\n",
    "\n",
    "#     # Particle Transformer (without interaction).\n",
    "#     print(f\" * Train Particle Transformer.\")\n",
    "#     name = train_benchmark(random_seed=random_seed, model_class=ParticleTransformer)\n",
    "#     name_list.append(name)\n",
    "\n",
    "#     # Particle Net.\n",
    "#     print(f\" * Train Particle Net.\")\n",
    "#     name = train_benchmark(random_seed=random_seed, model_class=ParticleNet)\n",
    "#     name_list.append(name)\n",
    "\n",
    "#     # QCGNN\n",
    "#     for num_nr_qubits in [3, 5, 7]:\n",
    "#         print(f\" * Train QCGNN {num_nr_qubits}.\")\n",
    "#         qcgnn_hparams = {'num_ir_qubits': 4, 'num_nr_qubits': num_nr_qubits, 'num_layers': 1, 'num_reupload': num_nr_qubits}\n",
    "#         name = train_quantum(random_seed=random_seed, model_class=QuantumRotQCGNN, model_hparams=qcgnn_hparams)\n",
    "#         name_list.append(name)\n",
    "    \n",
    "#     # Hybrid\n",
    "#     for num_nr_qubits in [3, 5, 7]:\n",
    "#         print(f\" * Train Hybrid {num_nr_qubits}.\")\n",
    "#         qcgnn_hparams = {'num_ir_qubits': 4, 'num_nr_qubits': num_nr_qubits, 'num_layers': 1, 'num_reupload': num_nr_qubits}\n",
    "#         name = train_quantum(random_seed=random_seed, model_class=HybridQCGNN, model_hparams=qcgnn_hparams)\n",
    "#         name_list.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Super MPGNN\n",
    "# print(f\" * Train MPGNN {1024}.\")\n",
    "# mpgnn_hparams = {'gnn_in': 6, 'gnn_out': 1024, 'gnn_layers': 2, 'gnn_hidden': 1024}\n",
    "# name = train_mpgnn(random_seed=0, model_hparams=mpgnn_hparams)\n",
    "\n",
    "# plot_metrics(name, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Particle Transformer (without interaction).\n",
    "# print(f\" * Train Particle Transformer.\")\n",
    "# name = train_benchmark(random_seed=0, model_class=ParticleTransformer)\n",
    "\n",
    "# plot_metrics(name, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Particle Net.\n",
    "# print(f\" * Train Particle Net.\")\n",
    "# name = train_benchmark(random_seed=0, model_class=ParticleNet)\n",
    "\n",
    "# plot_metrics(name, num_classes=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
