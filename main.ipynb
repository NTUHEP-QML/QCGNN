{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "\n",
    "from src.data.preprocessing import FatJetEvents\n",
    "from src.data.datamodule import JetTorchDataModule, JetGraphDataModule\n",
    "from src.models.qcgnn import QuantumRotQCGNN\n",
    "from src.models.mpgnn import ClassicalMPGNN\n",
    "from src.training.litmodel import TorchLightningModule, GraphLightningModel\n",
    "from src.training.loggers import csv_logger, wandb_logger\n",
    "\n",
    "with open('configs/config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    config['date'] = time.strftime('%Y%m%d_%H%M%S', time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_module(graph: bool) -> L.LightningDataModule:\n",
    "    \"\"\"Randomly create a data module.\"\"\"\n",
    "    \n",
    "    # Read jet data (reading is not affected by random seed).\n",
    "    sig_events = FatJetEvents(channel=config['Data']['sig'], **config['Data'])\n",
    "    bkg_events = FatJetEvents(channel=config['Data']['bkg'], **config['Data'])\n",
    "\n",
    "    # Randomly generated uniform events (will be affected by random seed).\n",
    "    sig_events = sig_events.generate_uniform_pt_events()\n",
    "    bkg_events = bkg_events.generate_uniform_pt_events()\n",
    "\n",
    "    # Turn into data module (for lightning training module).\n",
    "    if graph:\n",
    "        data_module = JetGraphDataModule(sig_events, bkg_events, **config['Data'])\n",
    "    else:\n",
    "        data_module = JetTorchDataModule(sig_events, bkg_events, **config['Data'])\n",
    "\n",
    "    return data_module\n",
    "\n",
    "\n",
    "def create_training_info(model: nn.Module, model_description: str, model_hparams: dict, random_seed: int) -> dict:\n",
    "    \"\"\"Create a training information dictionary that will be recorded.\"\"\"\n",
    "\n",
    "    # Hyperparameters and configurations that will be recorded.\n",
    "    training_info = {}\n",
    "    training_info.update(config['Data'])\n",
    "    training_info.update(config['Train'])\n",
    "    training_info.update(config['Model'])\n",
    "    training_info.update(model_hparams)\n",
    "\n",
    "    # Data information.\n",
    "    max_num_ptcs_per_jet = config['Data']['max_num_ptcs']\n",
    "    num_data_per_channel = config['Data']['num_bins'] * config['Data']['num_data_per_bin']\n",
    "    data_description = f\"Maxptc{max_num_ptcs_per_jet}_N{num_data_per_channel}\"\n",
    "\n",
    "    # Name of the model training.\n",
    "    name = '-'.join([\n",
    "        model.__class__.__name__,\n",
    "        model_description,\n",
    "        data_description,\n",
    "        f\"{config['date']}_{random_seed}\",\n",
    "    ])\n",
    "    \n",
    "    # Suffix of the training.\n",
    "    if config['Settings']['suffix'] != '':\n",
    "        name = name + '-' + config['Settings']['suffix']\n",
    "\n",
    "    training_info['model'] = model.__class__.__name__\n",
    "    training_info['random_seed'] = random_seed\n",
    "    training_info['model_description'] = model_description\n",
    "    training_info['data_description'] = data_description\n",
    "    training_info['name'] = name\n",
    "\n",
    "    return training_info\n",
    "\n",
    "    \n",
    "def create_lightning_model(model: nn.Module, graph: bool) -> L.LightningModule:\n",
    "    \"\"\"Create a lightning model for trainer.\"\"\"\n",
    "\n",
    "    # Optimizer.\n",
    "    lr = eval(config['Train']['lr'])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Create lightning model depends on graph or not.\n",
    "    score_dim = config['Model']['score_dim']\n",
    "    if graph:\n",
    "        return GraphLightningModel(model, optimizer=optimizer, score_dim=score_dim)\n",
    "    else:\n",
    "        return TorchLightningModule(model, optimizer=optimizer, score_dim=score_dim)\n",
    "    \n",
    "\n",
    "def create_trainer(model: nn.Module, training_info: dict, accelerator: str) -> L.Trainer:\n",
    "    \"\"\"Create lightning trainer for training.\"\"\"\n",
    "\n",
    "    # Create logger for monitoring the training.\n",
    "    if config['Settings']['use_wandb']:\n",
    "        wandb.login()\n",
    "        logger = wandb_logger(training_info)\n",
    "        logger.watch(model)\n",
    "    else:\n",
    "        logger = csv_logger(training_info)\n",
    "    \n",
    "    # Return the lightning trainer.\n",
    "    return L.Trainer(\n",
    "        logger=logger,\n",
    "        accelerator=accelerator,\n",
    "        max_epochs=config['Train']['max_epochs'],\n",
    "        log_every_n_steps=config['Train']['log_every_n_steps'],\n",
    "        num_sanity_val_steps=config['Train']['num_sanity_val_steps']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: nn.Module, model_description: str, model_hparams: dict,\n",
    "        random_seed: int, accelerator: str, graph: bool,\n",
    "    ):\n",
    "    \n",
    "    # Fix all random stuff.\n",
    "    L.seed_everything(random_seed)\n",
    "\n",
    "    # Traditional training procedure.\n",
    "    data_module = create_data_module(graph=graph)\n",
    "    lightning_model = create_lightning_model(model=model, graph=graph)\n",
    "    training_info = create_training_info(model, model_description, model_hparams, random_seed)\n",
    "    trainer = create_trainer(model, training_info, accelerator)\n",
    "    \n",
    "    trainer.fit(lightning_model, datamodule=data_module)\n",
    "\n",
    "    # Finish wandb if used.\n",
    "    if config['Settings']['use_wandb']:\n",
    "        wandb.finish()\n",
    "\n",
    "def train_qcgnn(random_seed: int, model_hparams: dict):\n",
    "\n",
    "    model_hparams.update(config['Model'])\n",
    "    model = QuantumRotQCGNN(**model_hparams)\n",
    "\n",
    "    num_ir_qubits = model_hparams['num_ir_qubits']\n",
    "    num_nr_qubits = model_hparams['num_nr_qubits']\n",
    "    num_layers = model_hparams['num_layers']\n",
    "    num_reupload = model_hparams['num_reupload']\n",
    "    model_description = f\"nI{num_ir_qubits}_nQ{num_nr_qubits}_l{num_layers}_r{num_reupload}\"\n",
    "\n",
    "    accelerator = 'cpu'\n",
    "    train(model, model_description, model_hparams, random_seed, accelerator, graph=False)\n",
    "\n",
    "def train_mpgnn(random_seed: int, model_hparams: dict):\n",
    "    \n",
    "    model_hparams.update(config['Model'])\n",
    "    model = ClassicalMPGNN(**model_hparams)\n",
    "\n",
    "    gnn_out = model_hparams['gnn_out']\n",
    "    gnn_hidden = model_hparams['gnn_hidden']\n",
    "    gnn_layers = model_hparams['gnn_layers']\n",
    "    model_description = f\"go{gnn_out}_gh{gnn_hidden}_gl{gnn_layers}\"\n",
    "\n",
    "    accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "    train(model, model_description, model_hparams, random_seed, accelerator, graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for random_seed in range(3):\n",
    "    print('=' * 10 + f\"Random Seed {random_seed}\" + '-' * 10)\n",
    "\n",
    "    # MPGNN\n",
    "    for gnn_dim in [3, 5, 7]:\n",
    "        print(f\" * Train MPGNN {gnn_dim}.\")\n",
    "        mpgnn_hparams = {'gnn_in': 6, 'gnn_out': gnn_dim, 'gnn_layers': 2, 'gnn_hidden': gnn_dim}\n",
    "        train_mpgnn(random_seed=random_seed, model_hparams=mpgnn_hparams)\n",
    "\n",
    "    # SUPER MPGNN\n",
    "    print(f\" * Train MPGNN {1024}.\")\n",
    "    mpgnn_hparams = {'gnn_in': 6, 'gnn_out': 1024, 'gnn_layers': 2, 'gnn_hidden': 1024}\n",
    "    train_mpgnn(random_seed=random_seed, model_hparams=mpgnn_hparams)\n",
    "\n",
    "    # QCGNN\n",
    "    for num_nr_qubits in [3, 5, 7]:\n",
    "        print(f\" * Train QCGNN {num_nr_qubits}.\")\n",
    "        qcgnn_hparams = {'num_ir_qubits': 4, 'num_nr_qubits': num_nr_qubits, 'num_layers': 1, 'num_reupload': num_nr_qubits}\n",
    "        train_qcgnn(random_seed=0, model_hparams=qcgnn_hparams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
